{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy\n",
    "SpaCy is fast and agile. It’s designed to amp up cutting edge NLP by making it practical and accessible. It works with other well-known libraries like Gensim and Scikit Learn. Written in Python and Cython, it’s optimized for performance and allows developers a more natural path to more advanced NLP tasks like named entity recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepareing Bibliography\n",
    "This is necessary to find the files attached in the Zotero Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybtex import database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Library:\n",
    "\n",
    "    def __init__(self, path, format='bibtex'):\n",
    "        self.path = path\n",
    "        self.library = database.parse_file(path, bib_format=format)\n",
    "        self.entries = []\n",
    "        for entry in self.library.entries:\n",
    "            self.entries.append(self.library.entries[entry])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "library = Library('/Users/paul/Desktop/FOM_MSc_Thesis.bib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print tiltles and paths for files in bibtexfile. count documents with filepaht\n",
    "\n",
    "len(library.entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entry('misc',\n",
       "  fields=[\n",
       "    ('title', 'Automating {Systematic} {Literature} {Reviews} with {Natural} {Language} {Processing} and {Text} {Mining}: a {Systematic} {Literature} {Review}'), \n",
       "    ('shorttitle', 'Automating {Systematic} {Literature} {Reviews} with {Natural} {Language} {Processing} and {Text} {Mining}'), \n",
       "    ('url', 'http://arxiv.org/abs/2211.15397'), \n",
       "    ('abstract', 'Objectives: An SLR is presented focusing on text mining based automation of SLR creation. The present review identifies the objectives of the automation studies and the aspects of those steps that were automated. In so doing, the various ML techniques used, challenges, limitations and scope of further research are explained. Methods: Accessible published literature studies that primarily focus on automation of study selection, study quality assessment, data extraction and data synthesis portions of SLR. Twenty-nine studies were analyzed. Results: This review identifies the objectives of the automation studies, steps within the study selection, study quality assessment, data extraction and data synthesis portions that were automated, the various ML techniques used, challenges, limitations and scope of further research. Discussion: We describe uses of NLP/TM techniques to support increased automation of systematic literature reviews. This area has attracted increase attention in the last decade due to significant gaps in the applicability of TM to automate steps in the SLR process. There are significant gaps in the application of TM and related automation techniques in the areas of data extraction, monitoring, quality assessment and data synthesis. There is thus a need for continued progress in this area, and this is expected to ultimately significantly facilitate the construction of systematic literature reviews.'), \n",
       "    ('urldate', '2023-12-29'), \n",
       "    ('publisher', 'arXiv'), \n",
       "    ('month', 'July'), \n",
       "    ('year', '2023'), \n",
       "    ('note', 'arXiv:2211.15397 [cs]'), \n",
       "    ('keywords', 'Computer Science - Information Retrieval'), \n",
       "    ('file', 'arXiv.org Snapshot:/Users/paul/Zotero/storage/PTRRNQPC/2211.html:text/html;Full Text PDF:/Users/paul/Zotero/storage/FL8H5YUJ/Sundaram und Berleant - 2023 - Automating Systematic Literature Reviews with Natural Language Processing and Text Mining a Systema.pdf:application/pdf')],\n",
       "  persons=OrderedCaseInsensitiveDict([('author', [Person('Sundaram, Girish'), Person('Berleant, Daniel')])]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "library.entries[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasse Dokument erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document():\n",
    "    base_path='/Users/paul/Zotero/storage/'\n",
    "\n",
    "    def __init__(self, entry):\n",
    "        self.entry = entry\n",
    "        self.title = self.entry.fields['title']\n",
    "        self.fields = self.entry.fields.keys()\n",
    "        if 'file' in self.fields:\n",
    "           self.file = self.entry.fields['file'].split('/Users/paul/Zotero/storage/')[1].split(':')[0]\n",
    "        else:\n",
    "            self.file = ''\n",
    "        self.text = None\n",
    "\n",
    "    def get_text(self, base_path=base_path):\n",
    "        self.text = extract_text(base_path+self.file)\n",
    "        return self.text\n",
    "\n",
    "    def split_sentences(self):\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s|(\\n){2,}', self.get_text())\n",
    "        sentences = [sentence.replace('\\n',' ') for sentence in sentences if sentence not in [None,'\\n','',' ','  ']]\n",
    "        return [sentence for sentence in sentences if not re.match(r'^[^a-zA-Z]*$', sentence)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The {PRISMA} 2020 statement: an updated guideline for reporting systematic reviews \n",
      "\tFilepath: 2PA9QMIU/Page et al. - 2021 - The PRISMA 2020 statement an updated guideline for reporting systematic reviews.pdf\n",
      "Location  where item  is reported  \n",
      "PRISMA 2020 Checklist \n",
      "Section and  Topic  \n",
      "Item  # \n",
      "Checklist item  \n",
      "TITLE   Title  \n",
      "ABSTRACT   Abstract  \n",
      "INTRODUCTION   Rationale  \n",
      "Identify the report as a systematic review.\n",
      "2  See the PRISMA 2020 for Abstracts checklist.\n",
      "3  Describe the rationale for the review in the context of existing knowledge.\n",
      "Objectives  \n",
      "4  Provide an explicit statement of the objective(s) or question(s) the review addresses.\n",
      "METHODS   Eligibility criteria  \n",
      "Information  sources  \n",
      "5  Specify the inclusion and exclusion criteria for the review and how studies were grouped for the syntheses.\n",
      "6  Specify all databases, registers, websites, organisations, reference lists and other sources searched or consulted to identify studies.\n",
      "Specify the \n",
      "date when each source was last searched or consulted.\n",
      "Search strategy \n",
      "7  Present the full search strategies for all databases, registers and websites, including any filters and limits used.\n",
      "Selection process \n",
      "8  Specify the methods used to decide whether a study met the inclusion criteria of the review, including how many reviewers screened each record \n",
      "and each report retrieved, whether they worked independently, and if applicable, details of automation tools used in the process.\n",
      "Data collection  process  \n",
      "9  Specify the methods used to collect data from reports, including how many reviewers collected data from each report, whether they worked \n",
      "independently, any processes for obtaining or confirming data from study investigators, and if applicable, details of automation tools used in the  process.\n",
      "Data items  \n",
      "10a  List and define all outcomes for which data were sought.\n",
      "Specify whether all results that were compatible with each outcome domain in each \n",
      "study were sought (e.g. for all measures, time points, analyses), and if not, the methods used to decide which results to collect.\n",
      "10b  List and define all other variables for which data were sought (e.g. participant and intervention characteristics, funding sources).\n",
      "Describe any \n",
      "assumptions made about any missing or unclear information.\n",
      "Study risk of bias  assessment \n",
      "11  Specify the methods used to assess risk of bias in the included studies, including details of the tool(s) used, how many reviewers assessed each \n",
      "study and whether they worked independently, and if applicable, details of automation tools used in the process.\n",
      "Effect measures  \n",
      "12  Specify for each outcome the effect measure(s) (e.g. risk ratio, mean difference) used in the synthesis or presentation of results.\n",
      "Synthesis  methods \n",
      "13a  Describe the processes used to decide which studies were eligible for each synthesis (e.g. tabulating the study intervention characteristics and \n",
      "comparing against the planned groups for each synthesis (item #5)).\n",
      "13b  Describe any methods required to prepare the data for presentation or synthesis, such as handling of missing summary statistics, or data \n",
      "conversions.\n",
      "13c  Describe any methods used to tabulate or visually display results of individual studies and syntheses.\n",
      "13d  Describe any methods used to synthesize results and provide a rationale for the choice(s).\n",
      "If meta-analysis was performed, describe the \n",
      "model(s), method(s) to identify the presence and extent of statistical heterogeneity, and software package(s) used.\n",
      "13e  Describe any methods used to explore possible causes of heterogeneity among study results (e.g. subgroup analysis, meta-regression).\n",
      "13f  Describe any sensitivity analyses conducted to assess robustness of the synthesized results.\n",
      "14  Describe any methods used to assess risk of bias due to missing results in a synthesis (arising from reporting biases).\n",
      "15  Describe any methods used to assess certainty (or confidence) in the body of evidence for an outcome.\n",
      "Reporting bias  assessment \n",
      "Certainty  assessment \n",
      "                                                  \fLocation  where item  is reported  \n",
      "PRISMA 2020 Checklist \n",
      "Section and  Topic  \n",
      "Item  # \n",
      "Checklist item  \n",
      "RESULTS   Study selection  \n",
      "Study  characteristics  \n",
      "Risk of bias in  studies  \n",
      "Results of  individual studies  \n",
      "Results of  syntheses \n",
      "16a  Describe the results of the search and selection process, from the number of records identified in the search to the number of studies included in \n",
      "the review, ideally using a flow diagram.\n",
      "16b  Cite studies that might appear to meet the inclusion criteria, but which were excluded, and explain why they were excluded.\n",
      "17  Cite each included study and present its characteristics.\n",
      "18  Present assessments of risk of bias for each included study.\n",
      "19  For all outcomes, present, for each study: (a) summary statistics for each group (where appropriate) and (b) an effect estimate and its precision \n",
      "(e.g. confidence/credible interval), ideally using structured tables or plots.\n",
      "20a  For each synthesis, briefly summarise the characteristics and risk of bias among contributing studies.\n",
      "20b  Present results of all statistical syntheses conducted.\n",
      "If meta-analysis was done, present for each the summary estimate and its precision (e.g. \n",
      "confidence/credible interval) and measures of statistical heterogeneity.\n",
      "If comparing groups, describe the direction of the effect.\n",
      "20c  Present results of all investigations of possible causes of heterogeneity among study results.\n",
      "20d  Present results of all sensitivity analyses conducted to assess the robustness of the synthesized results.\n",
      "Reporting biases \n",
      "21  Present assessments of risk of bias due to missing results (arising from reporting biases) for each synthesis assessed.\n",
      "Certainty of  evidence  \n",
      "DISCUSSION   Discussion  \n",
      "22  Present assessments of certainty (or confidence) in the body of evidence for each outcome assessed.\n",
      "23a  Provide a general interpretation of the results in the context of other evidence.\n",
      "23b  Discuss any limitations of the evidence included in the review.\n",
      "23c  Discuss any limitations of the review processes used.\n",
      "23d  Discuss implications of the results for practice, policy, and future research.\n",
      "OTHER INFORMATION  Registration and  protocol \n",
      "24a  Provide registration information for the review, including register name and registration number, or state that the review was not registered.\n",
      "24b \n",
      "Indicate where the review protocol can be accessed, or state that a protocol was not prepared.\n",
      "Support \n",
      "Competing  interests \n",
      "Availability of  data, code and  other materials \n",
      "24c  Describe and explain any amendments to information provided at registration or in the protocol.\n",
      "25  Describe sources of financial or non-financial support for the review, and the role of the funders or sponsors in the review.\n",
      "26  Declare any competing interests of review authors.\n",
      "27  Report which of the following are publicly available and where they can be found: template data collection forms; data extracted from included \n",
      "studies; data used for all analyses; analytic code; any other materials used in the review.\n",
      "From:    Page  MJ,  McKenzie  JE,  Bossuyt  PM,  Boutron  I,  Hoffmann  TC,  Mulrow  CD,  et  al.\n",
      " The  PRISMA  2020  statement:  an  updated  guideline  for  reporting  systematic  reviews.\n",
      " BMJ  2021;372:n71.\n",
      " doi:  10.1136/bmj.n71 \n",
      "For more information, visit: http://www.prisma-statement.org/  \n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "for entry in library.entries:\n",
    "    document = Document(entry)\n",
    "    documents.append(document)\n",
    "\n",
    "for doc in documents[-2:-1]:\n",
    "    print('Title: '+doc.title, \n",
    "          '\\n\\tFilepath: '+doc.file)\n",
    "    for sentence in doc.split_sentences():\n",
    "        print(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Text from PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    def __init__(self, sentence, file):\n",
    "        self.file = file\n",
    "        self.sentence = sentence \n",
    "        self.tokens = None\n",
    "        self.inventory = None\n",
    "        self.contains_noun = None\n",
    "        self.contains_verb = None\n",
    "        self.contains_cid = None\n",
    "        self.valid = None\n",
    "\n",
    "         #corp sentence to beginning based on first alphabtic character\n",
    "        for i, char in enumerate(self.sentence):\n",
    "            if char.isalpha():\n",
    "                self.sentence = self.sentence[i:]\n",
    "                break\n",
    "        \n",
    "        #replace tailing digits on words. those digits are usually footnotes\n",
    "        self.sentence = re.sub(r'[A-Za-z]\\d+\\b', '', self.sentence)\n",
    "\n",
    "    def tokenize(self):\n",
    "        self.tokens = [(word.text, word.pos_) for word in nlp(self.sentence)]\n",
    "    \n",
    "    def count_tokens(self):\n",
    "        if self.tokens is None:\n",
    "            self.tokenize()\n",
    "\n",
    "        inventory = {}\n",
    "        for _, value in self.tokens:\n",
    "            inventory[value] = inventory.get(value, 0) + 1\n",
    "    \n",
    "        self.inventory = inventory\n",
    "    \n",
    "    def check_validity(self):\n",
    "        if self.inventory is None:\n",
    "            self.count_tokens()\n",
    "\n",
    "        word_types = self.inventory.keys()\n",
    "\n",
    "        if 'NOUN' in word_types:\n",
    "            self.contains_noun = True\n",
    "        else:\n",
    "            self.contains_verb = False\n",
    "\n",
    "        if 'VERB' in word_types:\n",
    "            self.contains_verb = True\n",
    "        else:\n",
    "            self.contains_verb = False\n",
    "\n",
    "        if re.match(r'\\(cid:\\d{1,4}\\)', self.sentence):\n",
    "            self.contains_cid = True\n",
    "        \n",
    "        if self.contains_noun and self.contains_verb:\n",
    "            self.valid = True\n",
    "        else:\n",
    "            self.valid = False\n",
    "    \n",
    "    def summarize(self, show_token_details=False):\n",
    "        print(f'The origin file is: {self.file}')\n",
    "        print(f'The sentence is:\\n{self.sentence}')\n",
    "        print(f'The inventory holds:\\n{self.inventory}')\n",
    "        if show_token_details:\n",
    "            print(f'The token details are:\\n{self.tokens}')\n",
    "\n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sentences = []\n",
    "\n",
    "for sen in sentences:\n",
    "    Sen = Sentence(sen, file)\n",
    "    Sen.check_validity()\n",
    "    if Sen.valid:\n",
    "        valid_sentences.append(Sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in valid_sentences:\n",
    "    print(s.sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sentences[157].summarize(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Clustering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
