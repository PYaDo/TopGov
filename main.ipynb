{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy\n",
    "SpaCy is fast and agile. It’s designed to amp up cutting edge NLP by making it practical and accessible. It works with other well-known libraries like Gensim and Scikit Learn. Written in Python and Cython, it’s optimized for performance and allows developers a more natural path to more advanced NLP tasks like named entity recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepareing Bibliography\n",
    "This is necessary to find the files attached in the Zotero Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybtex import database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Library:\n",
    "\n",
    "    def __init__(self, path, format='bibtex'):\n",
    "        self.path = path\n",
    "        self.library = database.parse_file(path, bib_format=format)\n",
    "        self.entries = []\n",
    "        for entry in self.library.entries:\n",
    "            self.entries.append(self.library.entries[entry])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library = Library('/Users/paul/Desktop/FOM_MSc_Thesis.bib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library.entries[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    \n",
    "    def __init__(self, entry):\n",
    "        self.entry = entry\n",
    "        self.title = self.entry.fields['title']\n",
    "        self.fields = self.entry.fields.keys()\n",
    "        if 'file' in self.fields:\n",
    "           self.file = self.entry.fields['file'].split('/Users/paul/Zotero/storage/')[1].split(':application/')[0]\n",
    "        else:\n",
    "            self.file = None\n",
    "\n",
    "    def get_path(self):\n",
    "        if 'path' in self.fields:\n",
    "            return self.fields['path']\n",
    "        else:\n",
    "            None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for entry in library.entries:\n",
    "    document = Document(entry)\n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print tiltles and paths for files in bibtexfile. count documents with filepaht\n",
    "\n",
    "counter = 0\n",
    "for document in documents:\n",
    "    print(document.title, document.file)\n",
    "    if document.file is not None:\n",
    "        counter += 1\n",
    "print(counter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Text from PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/Users/paul/Zotero/storage/'\n",
    "file_paths = [document.file for document in documents if document.file is not None]\n",
    "file_paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text(base_path+file_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s|(\\n){2,}',text)\n",
    "sentences = [sentence.replace('\\n',' ') for sentence in sentences if sentence not in [None,'\\n','',' ','  ']]\n",
    "sentences = [sentence for sentence in sentences if not re.match(r'^[^a-zA-Z]*$', sentence)]\n",
    "for sentence in sentences:\n",
    "    print(repr(sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    def __init__(self, sentence, file):\n",
    "        self.file = file\n",
    "        self.sentence = sentence \n",
    "        self.tokens = None\n",
    "        self.inventory = None\n",
    "        self.contains_noun = None\n",
    "        self.contains_verb = None\n",
    "        self.contains_cid = None\n",
    "        self.valid = None\n",
    "\n",
    "         #corp sentence to beginning based on first alphabtic character\n",
    "        for i, char in enumerate(self.sentence):\n",
    "            if char.isalpha():\n",
    "                self.sentence = self.sentence[i:]\n",
    "                break\n",
    "        \n",
    "        #replace tailing digits on words. those digits are usually footnotes\n",
    "        self.sentence = re.sub(r'[A-Za-z]\\d+\\b', '', self.sentence)\n",
    "\n",
    "    def tokenize(self):\n",
    "        self.tokens = [(word.text, word.pos_) for word in nlp(self.sentence)]\n",
    "    \n",
    "    def count_tokens(self):\n",
    "        if self.tokens is None:\n",
    "            self.tokenize()\n",
    "\n",
    "        inventory = {}\n",
    "        for _, value in self.tokens:\n",
    "            inventory[value] = inventory.get(value, 0) + 1\n",
    "    \n",
    "        self.inventory = inventory\n",
    "    \n",
    "    def check_validity(self):\n",
    "        if self.inventory is None:\n",
    "            self.count_tokens()\n",
    "\n",
    "        word_types = self.inventory.keys()\n",
    "\n",
    "        if 'NOUN' in word_types:\n",
    "            self.contains_noun = True\n",
    "        else:\n",
    "            self.contains_verb = False\n",
    "\n",
    "        if 'VERB' in word_types:\n",
    "            self.contains_verb = True\n",
    "        else:\n",
    "            self.contains_verb = False\n",
    "\n",
    "        if re.match(r'\\(cid:\\d{1,4}\\)', self.sentence):\n",
    "            self.contains_cid = True\n",
    "        \n",
    "        if self.contains_noun and self.contains_verb:\n",
    "            self.valid = True\n",
    "        else:\n",
    "            self.valid = False\n",
    "    \n",
    "    def summarize(self, show_token_details=False):\n",
    "        print(f'The origin file is: {self.file}')\n",
    "        print(f'The sentence is:\\n{self.sentence}')\n",
    "        print(f'The inventory holds:\\n{self.inventory}')\n",
    "        if show_token_details:\n",
    "            print(f'The token details are:\\n{self.tokens}')\n",
    "\n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sentences = []\n",
    "\n",
    "for sen in sentences:\n",
    "    Sen = Sentence(sen, file)\n",
    "    Sen.check_validity()\n",
    "    if Sen.valid:\n",
    "        valid_sentences.append(Sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in valid_sentences:\n",
    "    print(s.sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sentences[157].summarize(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Clustering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
