{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found config files: ['init.cfg', 'locals.cfg']\n",
      "Missing files     : []\n",
      "Library folder path: /Users/paul/Documents/FOM/MasterArbeit/Thesis/dev/data/paper/paragraph\n",
      "Entries in library: 341\n"
     ]
    }
   ],
   "source": [
    "from config import config\n",
    "from Library import Library\n",
    "\n",
    "library = Library(config['bibliography']['file_path'], is_test=False, sample_size=None, source='paper', granularity='paragraph')\n",
    "print('Entries in library:', len(library.entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#library.delete_serialized_entries()\n",
    "library.deserialize()\n",
    "#library.deserialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = [\n",
    "            'data governance', 'data', 'governance', 'paper', 'research', 'policy', \n",
    "            'management', 'framework', 'government', \n",
    "            'utc', 'pp', 'free', 'mail', 'et', 'al', 'fig', 'ed', 'vol', \n",
    "            'citation', 'publication', 'review', 'question',\n",
    "            'license', 'authorized', 'restriction',\n",
    "            'library', 'academic', 'service', 'librarian',\n",
    "            'copy', 'permission', 'fee', 'copyright',\n",
    "            'universitatsbibliothek', 'licensed', 'utc'\n",
    "            ]\n",
    "library.set_stopwords(custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#library.save_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, years, embeddings = library.load_embeddings()\n",
    "print('Corpus:', len(years), '\\nYears:', len(corpus), '\\nEmbeddings:',  len(embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Function to store generated visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_metadata():\n",
    "\n",
    "    try:\n",
    "        umap_model_metadata = {\n",
    "            'n_neighbors': umap_model.n_neighbors,\n",
    "            'min_dist': umap_model.min_dist,\n",
    "            'n_components': umap_model.n_components,\n",
    "            'metric': umap_model.metric,\n",
    "            'random_state': umap_model.random_state\n",
    "        }\n",
    "    except NameError as ne:\n",
    "        umap_model_metadata = None\n",
    "        print(f'Error! N{str(ne)[1:]}.')\n",
    "\n",
    "    try:\n",
    "        hdbscan_model_metadata = {\n",
    "            'min_cluster_size': hdbscan_model.min_cluster_size,\n",
    "            'metric': hdbscan_model.metric,\n",
    "            'cluster_selection_method': hdbscan_model.cluster_selection_method,\n",
    "            'prediction_data': hdbscan_model.prediction_data\n",
    "        }\n",
    "    except NameError as ne:\n",
    "        hdbscan_model_metadata = None\n",
    "        print(f'Error! N{str(ne)[1:]}.')\n",
    "\n",
    "    try:\n",
    "        ctfidf_model_metadata = {\n",
    "            'reduce_frequent_words': ctfidf_model.reduce_frequent_words,\n",
    "            'seed_words': ctfidf_model.seed_words,\n",
    "            'seed_multiplier': ctfidf_model.seed_multiplier\n",
    "        }\n",
    "    except NameError as ne:\n",
    "        ctfidf_model_metadata = None\n",
    "        print(f'Error! N{str(ne)[1:]}.')\n",
    "\n",
    "\n",
    "    return {\n",
    "        'timestamp': datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S-%f\"),\n",
    "        'library.is_test': library.is_test,\n",
    "        'library.sample_size': library.sample_size,\n",
    "        'library.library_folder_path': library.library_folder_path,\n",
    "        'library.source_path': library.source_path,\n",
    "        'library.granularity': library.granularity,\n",
    "        'library.source': library.source,\n",
    "        'library.serialized_entries_path': library.serialized_entries_path,\n",
    "        'library.entries [count]': len(library.entries),\n",
    "        'library.docs [count]': len(library.docs),\n",
    "        'library.stopwords': library.stopwords,\n",
    "        'umap_model': umap_model_metadata,\n",
    "        'hdbscan_model': hdbscan_model_metadata,\n",
    "        'ctfidf_model': ctfidf_model_metadata\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import plotly\n",
    "import matplotlib\n",
    "import wordcloud\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def save_figure(fig, name, metadata: dict):\n",
    "\n",
    "    visuals_path = os.path.join(library.serialized_entries_path, 'visuals', name)\n",
    "    if not os.path.exists(visuals_path):\n",
    "        os.makedirs(visuals_path)\n",
    "\n",
    "\n",
    "\n",
    "    if isinstance(fig, matplotlib.figure.Figure):\n",
    "        filename = str(metadata['timestamp'])+'.png'\n",
    "        filepath = os.path.join(visuals_path, filename)\n",
    "        print('Saving figure to:', filepath)\n",
    "        fig.savefig(filepath)\n",
    "    elif isinstance(fig, plotly.graph_objs._figure.Figure):\n",
    "        filename = str(metadata['timestamp'])+'.html'\n",
    "        filepath = os.path.join(visuals_path, filename)\n",
    "        print('Saving figure to:', filepath)\n",
    "        fig.write_html(filepath)\n",
    "    elif isinstance(fig, wordcloud.wordcloud.WordCloud):\n",
    "        filename = str(metadata['timestamp'])+'.png'\n",
    "        filepath = os.path.join(visuals_path, filename)\n",
    "        print('Saving figure to:', filepath)\n",
    "        plt.imshow(fig, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.savefig(filepath)\n",
    "    else:\n",
    "        print('Unknown figure type:', type(fig), 'Cannot save figure.')\n",
    "        return\n",
    "\n",
    "\n",
    "    # if isinstance(fig, )\n",
    "\n",
    "    # Save the metadata to a JSON file\n",
    "    visuals_metadata_filename = os.path.join(visuals_path, 'visuals_metadata.json')\n",
    "\n",
    "\n",
    "    if os.path.exists(visuals_metadata_filename):\n",
    "        with open(visuals_metadata_filename, 'r') as file:\n",
    "            data = json.load(file)\n",
    "    else:\n",
    "        data = []\n",
    "\n",
    "    data.append(metadata)\n",
    "\n",
    "    with open(visuals_metadata_filename, 'w') as file:\n",
    "        json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow curve\n",
    "https://www.geeksforgeeks.org/elbow-method-for-optimal-value-of-k-in-kmeans/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "distortions = []\n",
    "inertias = []\n",
    "mapping1 = {}\n",
    "mapping2 = {}\n",
    "\n",
    "K = list(chain(range(2, 10), range(10, 50, 5), range(50, 101, 10)))\n",
    "print(K)\n",
    "\n",
    "for k in tqdm(K):\n",
    "\n",
    "    # Building and fitting the model\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(embeddings)\n",
    "    kmeanModel.fit(embeddings)\n",
    "    distortions.append(sum(np.min(cdist(embeddings, kmeanModel.cluster_centers_,\n",
    "                                        'euclidean'), axis=1)) / embeddings.shape[0]) \n",
    "    inertias.append(kmeanModel.inertia_)\n",
    "    mapping1[k] = sum(np.min(cdist(embeddings, kmeanModel.cluster_centers_,\n",
    "                                   'euclidean'), axis=1)) / embeddings.shape[0]\n",
    "    mapping2[k] = kmeanModel.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_distortions = plt.figure(figsize=(7, 2))\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('Values of K')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method using Distortion')\n",
    "plt.show()\n",
    "save_figure(fig_distortions, 'elbow_distortion', get_metadata())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig_inertias = plt.figure(figsize=(7, 2))\n",
    "plt.plot(K, inertias, 'bx-')\n",
    "plt.xlabel('Values of K')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('The Elbow Method using Inertia')\n",
    "plt.show()\n",
    "save_figure(fig_inertias, 'elbow_inertia', get_metadata())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silhouette Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette analysis\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "for k in [2, 5, 10, 15, 20, 25]:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(embeddings) + (k + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    cluster_labels = kmeanModel.predict(embeddings)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(embeddings, cluster_labels)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        k,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(embeddings, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(k):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / k)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / k)\n",
    "    ax2.scatter(\n",
    "        embeddings[:, 0], embeddings[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = kmeanModel.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(\n",
    "        centers[:, 0],\n",
    "        centers[:, 1],\n",
    "        marker=\"o\",\n",
    "        c=\"white\",\n",
    "        alpha=1,\n",
    "        s=200,\n",
    "        edgecolor=\"k\",\n",
    "    )\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n",
    "        % k,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model for Dimensionality Reduction\n",
    "\n",
    "from umap import UMAP\n",
    "\n",
    "umap_model = UMAP(n_neighbors=10,   # Controls the size of the local neighborhood around each data point. \n",
    "                                    # Smaller values will lead to more local representations, while larger \n",
    "                                    # values will capture more global structure. \n",
    "                  n_components=10,  # Determine the dimensionality of the reduced dimension space.\n",
    "                  min_dist=0.0, \n",
    "                  metric='cosine', \n",
    "                  random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up clustering model\n",
    "\n",
    "from hdbscan import HDBSCAN\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=50,  # The smallest size grouping that should be considered a cluster.\n",
    "                        metric='euclidean', \n",
    "                        cluster_selection_method='eom', \n",
    "                        #prediction_data=True   # Whether to construct a fuzzy membership vector for each data point.\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Fine-tune topic representations after training BERTopic\n",
    "vectorizer_model = CountVectorizer(stop_words='english', ngram_range=(1, 2), min_df=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model for Class-based TF-IDF\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "ctfidf_model = ClassTfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure AI Client Setup (for Chat GPT usage)\n",
    "\n",
    "from config import config\n",
    "from openai import AzureOpenAI\n",
    "    \n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=config['azure']['endpoint'],\n",
    "    api_key=config['azure']['api_key'],\n",
    "    api_version=\"2024-02-01\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic.representation import OpenAI\n",
    "\n",
    "label_prompt =  \"\"\"\n",
    "                    ---- CONTEXT ----\n",
    "                    You are an AI Assistant that helps people assigning topics to a collection of the following paragraphs:\n",
    "                    [DOCUMENTS].\n",
    "\n",
    "                    The topic is described by the following keywords:\n",
    "                    [KEYWORDS]\n",
    "                    \n",
    "                    ---- TASK ----\n",
    "                    The maximum word count for the topic label is 2 words. Please assign a topic.\n",
    "\n",
    "                    ---- EXAMPLE RESULT ----\n",
    "                    Topic Label\n",
    "                \"\"\"\n",
    "\n",
    "label_model = OpenAI(client=client, delay_in_seconds=3, chat=True, model='gpt-4', prompt=label_prompt, nr_docs=10, tokenizer='vectorizer', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_model = {\n",
    "    'Main': None,\n",
    "    #'Label': label_model   # Use the label model for the topic representation.\n",
    "                            # If uncommented, the label model will not be used for the topic representation.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "# Initialize BERTopic as Topic Model\n",
    "topic_model = BERTopic(\n",
    "                    umap_model=umap_model\n",
    "                    ,hdbscan_model=hdbscan_model\n",
    "                    ,vectorizer_model=vectorizer_model\n",
    "                    ,ctfidf_model=ctfidf_model\n",
    "                    ,representation_model=representation_model\n",
    "                    )\n",
    "topics, probs = topic_model.fit_transform(corpus, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principle_prompt =  \"\"\" \n",
    "                        ---- CONTEXT ----\n",
    "                        You are an AI Assistant that helps people summarizing [KEYWORDS] into a short [PRINCIPLE].\n",
    "                        Your advice needs to be very concise and very short. \n",
    "\n",
    "                        ---- TASK ----\n",
    "                        Draft one Principle based on the given [KEYWORDS].\n",
    "\n",
    "                    \"\"\"\n",
    "\n",
    "framwork_prompt = \"\"\"   \n",
    "                        ---- CONTEXT ----\n",
    "                        You are an AI Assistant that helps people assinging a defined [LABEL] to the given keywords. Each [LABEL] is followed by a short discription. \n",
    "                        \n",
    "                        ---- LABELS & DESCRIPTION ----\n",
    "                        [Antecedents]: external - legal and regulatory requirements, market volaitlity, industry, country; internal - organizationa strategy, it strategy, diversification breadth, it achitecture, organizational culture, senior management support\n",
    "                        [Data Scope]: traditional data - master data, transactional data, reference data; big data - web and social media data, machine generated data, streaming data, biometric data\n",
    "                        [Domain Scope]: data quality, data security, data architecture, data lifecycle, meta data, data storage and infrastructure\n",
    "                        [Organizational Scope]: intra-organizational - data governance on project-level / firm-level; inter-organizational - data governance between firms / ecosystems\n",
    "                        [Governance Mechanisms]: structural mechanisms - roles and responsibilites, location of decision-making authority ; proecedural mechanisms - policies, standards, processes, procedures, contractual agreements, performance measurement, compliance monitoring, issue management; relational mechanisms - communication, training, coordination of decision-making\n",
    "                        [Consequences]: intermediate performance effects, risk management\n",
    "                        [Other] \n",
    "\n",
    "                        ---- RESULT ----\n",
    "                        The potential [LABELS] are:\n",
    "                        \n",
    "                        [Antecedents]\n",
    "                        [Data Scope]\n",
    "                        [Domain Scope]\n",
    "                        [Organizational Scope]\n",
    "                        [Governance Mechanisms]\n",
    "                        [Consequences]\n",
    "                        [Other]\n",
    "\n",
    "                        Each prompt should return just on of the potential labels. Anything other but the [LABEL] is prohibited.\n",
    "                        An example result could be: \"Data Scope\"\n",
    "\n",
    "                    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get a response from ChatGPT.\n",
    "def query_gpt(prompt, keywords):\n",
    "\n",
    "    def flatten_list(nested_list):\n",
    "        flattened = []\n",
    "        for element in nested_list:\n",
    "            if isinstance(element, list):\n",
    "                flattened.extend(flatten_list(element))\n",
    "            else:\n",
    "                flattened.append(element)\n",
    "        return flattened\n",
    "\n",
    "    flattened = flatten_list(keywords)\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{', '.join(flattened)}\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "topics = topic_model.get_topic_info()\n",
    "\n",
    "# Reformat Label and Representation\n",
    "topics['Representation'] = topics['Representation'].apply(lambda x: ', '.join(x))\n",
    "\n",
    "# def extract_first_sublist(input_str):\n",
    "#     # Use a regular expression to find all substrings enclosed in square brackets\n",
    "#     sublists = re.findall(r'\\[.*?\\]', input_str)\n",
    "    \n",
    "#     # Return the first sublist, if any are found\n",
    "#     return sublists[0] if sublists else None\n",
    "\n",
    "topics['Label'] = topics['Label'].apply(lambda x: x[0])\n",
    "\n",
    "# Classify Area based on Representation\n",
    "topics['Area'] = topics.apply(lambda x: query_gpt(framwork_prompt , [x.Representation, x.Label]).strip['['].strip(']').strip('\"').strip('.'), axis=1)\n",
    "\n",
    "# Generate Principle for each Cluster\n",
    "topics['Principle'] = topics.apply(lambda x: query_gpt(principle_prompt, [x.Representation, x.Label]).strip('\"') if x['Area'] != 'Other' else '', axis=1)\n",
    "\n",
    "# Set topic ID starting with 1.\n",
    "topics['ID'] = pd.Series(range(0, len(topics))) \n",
    "\n",
    "# Show topics table results\n",
    "pd.set_option('display.max_rows', topics.shape[0]+1)\n",
    "topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "# Create directory for storing the results.\n",
    "if not os.path.exists(os.path.join(library.serialized_entries_path, 'results')):\n",
    "    os.makedirs(os.path.join(library.serialized_entries_path, 'results'), exist_ok=True)\n",
    "\n",
    "## Commented to avoid overwriting the results\n",
    "# topics = topics[topics['Topic']!=-1] # Exlude noise with topic ID -1\n",
    "# topics.to_csv(\n",
    "#     os.path.join(library.serialized_entries_path, \n",
    "#     'results', 'topics.csv'), sep=';', \n",
    "#     index=False\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualization of paragraphs per area and year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "loaded_topics = pd.read_csv(config['results']['topics'], sep=';')\n",
    "\n",
    "# Assuming your dataframe is named 'df' and the two columns are 'column1' and 'column2'\n",
    "topic_dict = {key: value for key, value in zip(loaded_topics['Topic'], loaded_topics['Area'])}\n",
    "\n",
    "apd = [topic_dict.get(item) for item in topic_model.topics_] #documents per area for each year\n",
    "yapd = list(zip(apd, years)) #year, documents per area\n",
    "yapd\n",
    "\n",
    "# List of tuples\n",
    "data = yapd\n",
    "\n",
    "# Grouping and counting\n",
    "counts = defaultdict(lambda: defaultdict(int))\n",
    "for item in data:\n",
    "    if item[0] == None:\n",
    "        counts['Unassigned'][item[1]] += 1\n",
    "    else:\n",
    "        counts[item[0]][item[1]] += 1\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "areas = ['Other', 'Data Scope', 'Domain Scope', 'Organizational Scope', 'Governance Mechanisms', 'Antecedents', 'Consequences']\n",
    "\n",
    "selected_counts = { key: value for key, value in counts.items() if key in areas }\n",
    "selected_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "area_counts = {}\n",
    "year_ax = tuple(sorted(set(years)))\n",
    "\n",
    "default_dict = {}\n",
    "for year in year_ax:\n",
    "        default_dict[year] = 0\n",
    "\n",
    "# Printing the counts\n",
    "for area_name, area_dict in selected_counts.items():\n",
    "\n",
    "    sub_dict = default_dict.copy()\n",
    "\n",
    "    for year, counted in area_dict.items():\n",
    "        sub_dict[year] = counted\n",
    "        sorted_dict = dict(sorted(sub_dict.items()))\n",
    "\n",
    "    area_counts[area_name] = np.array(list(sorted_dict.values()))\n",
    "\n",
    "# Summing up the counts for all areas\n",
    "paragraph_counts = sum(area_counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "year_dt = mdates.datestr2num(year_ax)\n",
    "\n",
    "coefficients = np.polyfit(year_dt, paragraph_counts, 1)\n",
    "reg_line = np.poly1d(coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('white')\n",
    "\n",
    "pal = sns.color_palette(\"Greens_d\", len(area_counts))\n",
    "\n",
    "width = 0.5\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bottom = np.zeros(15)\n",
    "\n",
    "for boolean, area_count in area_counts.items():\n",
    "    p = ax.bar(year_ax, area_count, width, label=boolean, bottom=bottom)\n",
    "    bottom += area_count\n",
    "\n",
    "ax.plot(year_ax, reg_line(year_dt), '--', label='Trend', color='black')\n",
    "\n",
    "ax.set_title(\"Count of validated paragraphs per year and area\")\n",
    "ax.legend(loc=\"upper left\", title=\"Area\", frameon=False)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "def create_wordcloud(model, topic):\n",
    "    text = {word: value for word, value in model.get_topic(topic)}\n",
    "    wc = WordCloud(background_color=\"white\", max_words=1000)\n",
    "    wc.generate_from_frequencies(text)\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    #plt.show()\n",
    "    return wc\n",
    "\n",
    "topic_ids = topic_model.get_topic_info()['Topic'].values\n",
    "\n",
    "for id in topic_ids:\n",
    "    wc = create_wordcloud(topic_model, topic=id)\n",
    "    #print(type(wc))\n",
    "    md = get_metadata()\n",
    "    save_figure(wc, f'wordcloud_topic', md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hierarchical topics and their representations\n",
    "hierarchical_topics = topic_model.hierarchical_topics(corpus)\n",
    "\n",
    "# Visualize these representations\n",
    "topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = topic_model.visualize_barchart(n_words=6, )\n",
    "bc.show()\n",
    "save_figure(bc, 'barchart', get_metadata())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = topic_model.visualize_topics()\n",
    "tc.show()\n",
    "save_figure(tc, 'topicchart', get_metadata())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm = topic_model.visualize_heatmap()\n",
    "hm.show()\n",
    "save_figure(hm, 'heatmap', get_metadata())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://maartengr.github.io/BERTopic/getting_started/topicsovertime/topicsovertime.html\n",
    "\n",
    "tot = topic_model.topics_over_time(corpus, years)\n",
    "topic_ids = range(0, 20)\n",
    "vtot = topic_model.visualize_topics_over_time(tot, topics=topic_ids)\n",
    "vtot.show()\n",
    "save_figure(vtot, 'topics_over_time', get_metadata())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Data Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing dimensionality to 2d -> Play around with n_neighbors and min_dist\n",
    "# Note that these 2D embeddings are very sensitive to hyperparameters\n",
    "\n",
    "umap_embeddings = UMAP( n_neighbors=15, # Controls the size of the local neighborhood around each data point. \n",
    "                                        # Smaller values will lead to more local representations, while larger \n",
    "                                        # values will capture more global structure. \n",
    "                        n_components=2, # Determine the dimensionality of the reduced dimension space.\n",
    "                        min_dist=0.0, \n",
    "                        metric='cosine', \n",
    "                        random_state=42\n",
    "                        ).fit_transform(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# Combine data\n",
    "df = pd.DataFrame(umap_embeddings, columns=['x', 'y'])\n",
    "df['topic'] = pd.Series(topic_model.topics_)\n",
    "\n",
    "# Visualize static clusters\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "outliers = df.loc[df.topic <= 0, :] # largest 2 clusters are consired not distinct enough and therfore removed from visualization.\n",
    "clustered = df.loc[df.topic >0, :]\n",
    "\n",
    "#plt.scatter(outliers.x, outliers.y, color='#BEBEBE', s=.5) # Grey color for outliers\n",
    "plt.scatter(clustered.x, clustered.y, c=clustered.topic, s=5, cmap=plt.cm.viridis)\n",
    "plt.colorbar(ticks=range(len(set(clustered.topic))+1), label='Cluster')\n",
    "plt.axis('off')\n",
    "plt.clim(-0.5, (len(set(clustered.topic))+.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary from two columns\n",
    "labels = topics.set_index('Topic')['Label'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_sizes = pd.Series(topic_model.topics_).value_counts()\n",
    "topic_sizes.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter topics based on conditions\n",
    "filter_topics = []\n",
    "\n",
    "filter_topics.extend([-1]) # Remove outliers\n",
    "\n",
    "# Remove topics with more than 2000 documents\n",
    "filter_topics.extend(list(topic_sizes[topic_sizes > 2000].index)) \n",
    "\n",
    "# Remove topics with less than 70 documents\n",
    "filter_topics.extend(list(topic_sizes[topic_sizes < 70].index))\n",
    "\n",
    "# Remove topics with specific names\n",
    "filter_names = ['download', 'classroom', 'article', 'food', '- hide -']\n",
    "filter_topics.extend([key for key, value in labels.items() if value in filter_names]) \n",
    "\n",
    "flter_ids = [8,57]\n",
    "filter_topics.extend(flter_ids)\n",
    "\n",
    "filter_topics = set(filter_topics)\n",
    "filter_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_topics = pd.Series(topic_model.topics_)\n",
    "\n",
    "# Convert topic ids to topic labels based on label dictionary\n",
    "simplified_topics = [\n",
    "                    (topicID, labels[topicID]) \n",
    "                    if topicID not in filter_topics\n",
    "                    else (topicID, '- hide -')\n",
    "                    for topicID in simplified_topics\n",
    "                    ]\n",
    "\n",
    "# Print sample of simplified topics\n",
    "simplified_topics[1:100:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def filter_topics(embeddings, topics):\n",
    "\n",
    "    filtered_embeddings = []\n",
    "    filtered_topics = []\n",
    "\n",
    "    for embedding, topic in zip(embeddings, topics):\n",
    "        id, keyword = topic\n",
    "        \n",
    "        if keyword != '- hide -':\n",
    "            filtered_topics.append(': '.join([str(id+1), keyword])) # Add 1 to topic id to start from 1 after omitting -1 for outliers\n",
    "            filtered_embeddings.append(embedding)\n",
    "    \n",
    "    return np.vstack(filtered_embeddings), filtered_topics\n",
    "\n",
    "\n",
    "filtered_umap_embeddings, selected_topics = filter_topics(\n",
    "    umap_embeddings, \n",
    "    simplified_topics, \n",
    "    #for visualization purposes, we filter out the topics that are outliers or most likely not relevant\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data map plot in WordCloud style using datamapplot-library\n",
    "import datamapplot\n",
    "import matplotlib.cm as cm\n",
    "wmp, ax = datamapplot.create_plot(\n",
    "    filtered_umap_embeddings, \n",
    "    selected_topics,\n",
    "    noise_color='white',\n",
    "    noise_label='- hide -',\n",
    "    label_over_points=True,\n",
    "    # dynamic_label_size=True,\n",
    "    # dynamic_label_size_scaling_factor=0.5,\n",
    "    # max_font_size=12,\n",
    "    # min_font_size=4,\n",
    "    # max_font_weight=200,\n",
    "    # min_font_weight=100,\n",
    "    darkmode=False,\n",
    "    color_label_text=True,\n",
    "    font_family='Helvetica',\n",
    "    cmap=cm.viridis,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figure(wmp, 'datamapplot', get_metadata())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Document Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_hierarchy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(set(key for entry in library.entries for key in entry.fields.keys()))\n",
    "\n",
    "result = {}\n",
    "for key in keys:\n",
    "    for entry in library.entries:\n",
    "        if key in entry.fields.keys():\n",
    "            if key in result.keys():\n",
    "                result[key] += 1\n",
    "            else:\n",
    "                result[key] = 1\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [entry.fields['year'] for entry in library.entries]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the number of documents per year\n",
    "document_count = {}\n",
    "for year in years:\n",
    "    if year in document_count:\n",
    "        document_count[year] += 1\n",
    "    else:\n",
    "        document_count[year] = 1\n",
    "\n",
    "count_sorted = {}\n",
    "for key in sorted(document_count.keys()):\n",
    "    count_sorted[key] = document_count[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpy, ax = plt.subplots()\n",
    "\n",
    "# Create a bar plot\n",
    "ax.bar(count_sorted.keys(), count_sorted.values())\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Document Count')\n",
    "ax.set_title('Number of Documents per Year')\n",
    "ax.tick_params(axis='x', rotation=90)\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "dpy.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figure(dpy, 'document_count_per_year', get_metadata())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
