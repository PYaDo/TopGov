{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy\n",
    "SpaCy is fast and agile. It’s designed to amp up cutting edge NLP by making it practical and accessible. It works with other well-known libraries like Gensim and Scikit Learn. Written in Python and Cython, it’s optimized for performance and allows developers a more natural path to more advanced NLP tasks like named entity recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepareing Bibliography\n",
    "This is necessary to find the files attached in the Zotero Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybtex import database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Library:\n",
    "\n",
    "    def __init__(self, path, format='bibtex'):\n",
    "        self.path = path\n",
    "        self.library = database.parse_file(path, bib_format=format)\n",
    "        self.entries = []\n",
    "        for entry in self.library.entries:\n",
    "            self.entries.append(self.library.entries[entry])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "library = Library('/Users/paul/Desktop/FOM_MSc_Thesis.bib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print tiltles and paths for files in bibtexfile. count documents with filepaht\n",
    "\n",
    "len(library.entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entry('misc',\n",
       "  fields=[\n",
       "    ('title', 'Automating {Systematic} {Literature} {Reviews} with {Natural} {Language} {Processing} and {Text} {Mining}: a {Systematic} {Literature} {Review}'), \n",
       "    ('shorttitle', 'Automating {Systematic} {Literature} {Reviews} with {Natural} {Language} {Processing} and {Text} {Mining}'), \n",
       "    ('url', 'http://arxiv.org/abs/2211.15397'), \n",
       "    ('abstract', 'Objectives: An SLR is presented focusing on text mining based automation of SLR creation. The present review identifies the objectives of the automation studies and the aspects of those steps that were automated. In so doing, the various ML techniques used, challenges, limitations and scope of further research are explained. Methods: Accessible published literature studies that primarily focus on automation of study selection, study quality assessment, data extraction and data synthesis portions of SLR. Twenty-nine studies were analyzed. Results: This review identifies the objectives of the automation studies, steps within the study selection, study quality assessment, data extraction and data synthesis portions that were automated, the various ML techniques used, challenges, limitations and scope of further research. Discussion: We describe uses of NLP/TM techniques to support increased automation of systematic literature reviews. This area has attracted increase attention in the last decade due to significant gaps in the applicability of TM to automate steps in the SLR process. There are significant gaps in the application of TM and related automation techniques in the areas of data extraction, monitoring, quality assessment and data synthesis. There is thus a need for continued progress in this area, and this is expected to ultimately significantly facilitate the construction of systematic literature reviews.'), \n",
       "    ('urldate', '2023-12-29'), \n",
       "    ('publisher', 'arXiv'), \n",
       "    ('month', 'July'), \n",
       "    ('year', '2023'), \n",
       "    ('note', 'arXiv:2211.15397 [cs]'), \n",
       "    ('keywords', 'Computer Science - Information Retrieval'), \n",
       "    ('file', 'arXiv.org Snapshot:/Users/paul/Zotero/storage/PTRRNQPC/2211.html:text/html;Full Text PDF:/Users/paul/Zotero/storage/FL8H5YUJ/Sundaram und Berleant - 2023 - Automating Systematic Literature Reviews with Natural Language Processing and Text Mining a Systema.pdf:application/pdf')],\n",
       "  persons=OrderedCaseInsensitiveDict([('author', [Person('Sundaram, Girish'), Person('Berleant, Daniel')])]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "library.entries[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasse Dokument erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document():\n",
    "    base_path='/Users/paul/Zotero/storage/'\n",
    "\n",
    "    def __init__(self, entry):\n",
    "        self.entry = entry\n",
    "        self.title = self.entry.fields['title']\n",
    "        self.fields = self.entry.fields.keys()\n",
    "        if 'file' in self.fields:\n",
    "           self.file = self.entry.fields['file'].split('/Users/paul/Zotero/storage/')[1].split(':')[0]\n",
    "        else:\n",
    "            self.file = ''\n",
    "        self.is_pdf = re.search('.pdf', self.file)\n",
    "        self.text = None\n",
    "\n",
    "    def get_text(self, base_path=base_path):\n",
    "        self.text = extract_text(base_path+self.file)\n",
    "        return self.text\n",
    "\n",
    "    def split_sentences(self):\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s|(\\n){2,}', self.get_text())\n",
    "        sentences = [sentence.replace('\\n',' ') for sentence in sentences if sentence not in [None,'\\n','',' ','  ']]\n",
    "        return [sentence for sentence in sentences if not re.match(r'^[^a-zA-Z]*$', sentence)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for entry in library.entries:\n",
    "    document = Document(entry)\n",
    "    if document.is_pdf:\n",
    "        documents.append(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: An automated method for developing search strategies for systematic review using {Natural} {Language} {Processing} ({NLP}) \n",
      "\tFilepath: 2W9KB3PQ/Kwabena et al. - 2023 - An automated method for developing search strategies for systematic review using Natural Language Pr.pdf\n",
      "['MethodsX 10 (2023) 101935 ', 'Contents lists available at ScienceDirect ', 'MethodsX ', 'journal homepage: www.elsevier.com/locate/mex ', 'Method Article ', 'An automated method for developing search strategies for  systematic review using Natural Language Processing (NLP) ', 'Antwi Eﬀah Kwabena a , ∗ , Owusu-Banahene Wiafe b , Boakye-Danquah John a ,  Asare Bernard b , Frimpong A.F. Boateng b  a  Canadian Forest Service, Great Lakes Forestry Centre, 1219 Queen Street East, Sault Ste.', 'Marie, Ontario, P6A 2E5  b  University of Ghana, Department of Computer Engineering, P.O. BOX LG 77, Legon, Accra, Ghana ', 'a r t i c l e ', 'i n f o ', 'a b s t r a c t ', 'Method name:  Search Strategy  Search Terms  Data Deduplication  Software Implementation  Evidence Synthesis  Systematic Literature Review  Text mining and keyword co ‐occurrence  networks to identify the most important terms  for a review ', 'The design and implementation of systematic reviews and meta-analyses are often hampered by  high ﬁnancial costs, signiﬁcant time commitment, and biases due to researchers’ familiarity with  studies.', 'We proposed and implemented a fast and standardized method for search term selection  using Natural Language Processing (NLP) and co-occurrence networks to identify relevant search  terms to reduce biases in conducting systematic reviews and meta-analyses.', '• The method was implemented using Python packaged dubbed Ananse, which is benchmarked  on the search terms strategy for naïve search proposed by Grames et al.', '(2019) written in “R ”.', ' Ananse was applied to a case example towards ﬁnding search terms to implement a systematic  literature review on cumulative eﬀect studies on forest ecosystems.', '• The software automatically corrected and classiﬁed 100% of the duplicate articles identiﬁed  by manual deduplication.', 'Ananse was applied to the cumulative eﬀects assessment case study,  but it can serve as a general-purpose, open-source software system that can support extensive  systematic reviews within a relatively short period with reduced biases.', '• Besides generating keywords, Ananse can act as middleware or a data converter for integrating ', 'multiple datasets into a database.', 'Corresponding Author.', ' E-mail address: eﬀah.antwi@canada.ca (A.E. Kwabena) .', 'https://doi.org/10.1016/j.mex.2022.101935  Received 15 November 2021; Accepted 18 November 2022  Available online 23 November 2022  2215-0161/Crown Copyright © 2022 Published by Elsevier B.V. This is an open access article under the CC BY license  ( http://creativecommons.org/licenses/by/4.0/ ) ', '\\x0cA.E. Kwabena, O.-B.', 'Wiafe, B.-D.', 'John et al.', 'Speciﬁcations table ', 'MethodsX 10 (2023) 101935 ', 'Subject Area  More speciﬁc subject area  Method name ', 'Name and reference of original  method ', 'Resource availability ', 'Environmental Science  Evidence synthesis in environmental and biological sciences  Text mining and keyword co ‐occurrence networks to identify the most important terms for a  review  Grames, E.', 'M., Stillman, A.', 'N., Tingley, M.', 'W., & Elphick, C.', 'S.', 'An automated  approach to identifying search terms for systematic reviews using keyword  co ‐occurrence networks.', 'Methods in Ecology and Evolution, 10(10), 1645-1654.', ' Documentation: https://baasare.github.io/ananse/_build/html/index.html  Software: GitHub - baasare/ananse  Method description: ananse · PyPI ', 'Background ', 'Historically, summaries of scientiﬁc evidence have helped discover patterns of phenomena, develop theories or concepts, and  inform practice.', 'Although common with editors and readers alike, this approach is less rigorous since evidence summarized this way  is less likely to answer speciﬁc clinical questions and more likely to contain literature selected by the authors and recommendations  prejudiced strongly by opinion.', 'With exponential growth in scientiﬁc literature, the search for a structured and eﬀective evidence  synthesis has become a critical scientiﬁc endeavor.', 'Evidence synthesis involves combining information from multiple studies or  research that have investigated the same or similar issue to come to a conclusive understanding of a speciﬁc topic [1] .', 'It often  involves summarizing trends, identifying emerging questions, and clarifying disagreements and conﬂicting results [ 2 , 3 ].', 'Since 1753 when James Lind published the ﬁrst evidence synthesis to provide a concise and unbiased summary of evidence  on scurvy, improvement in the state of evidence synthesis has grown [ 4 , 5 ].', 'In the past two decades, advances in computer-aided  technology have enabled the growth and development of various forms of evidence synthesis.', 'The two central techniques known  to have originated from the medical sciences and are commonly used today to synthesize evidence are systematic reviews (SRs) – which search available literature for evidence that addresses the research question, - and meta-analyses – which quantitatively assess  statistical evidence found through systematic reviews [5] .', 'Evolutionary and behavioral ecologists started adopting meta-analyses  in the mid-1990s and became fully embraced since 2010 [6] .', 'Meta-analysis has since become the gold standard for combining  information from multiple studies across disciplines.', 'However, a good meta-analysis is dependent on a good sampling of the core  universe of studies, thus requiring a careful and comprehensive SR.', 'A SR involves the review of an articulated research question  using systematic and testable methods to help to identify, select and evaluate all pertinent research [7] , and collect and analyze data  from the studies that are included in the review [8] .', 'An excellent SR assembles and presents an impartial and objective summary of  ﬁndings, assesses all results for inclusion/exclusion and quality, and minimizes bias at all stages of the process [7] .', 'However, the process of evidence synthesis is very tedious and often involves experienced methodologists and disciplinary experts  combing through all relevant studies, both published and unpublished, through a guided methodological process.', 'As such, it tends  to be costly and tedious as it can take months, or even years, to complete, making it practically challenging [9] .', 'According to  some estimates, conducting a SR can take up to 2 years to complete.', '[10] also suggest that the time needed to complete a SR with  meta-analyses ranges from 216 to 2,518 hours.', 'According to [11] , conducting an eﬀective systematic search requires an information  specialist’s expertise and time, who need an average aggregated time of 26.9 hours when developing a search strategy.', 'Thus, the  design and implementation of evidence-based synthesis are hampered by high ﬁnancial costs [3] and signiﬁcant time commitment  [2] .', 'To overcome time and resource constraints required to synthesize evidence, scholars have adopted automation of the laborious  tasks in SR [12] .', 'Advances in computer-aided technology have helped automate aspects of the evidence synthesis process to improve  eﬃciency and cut costs and time while still maintaining the standards of conventional search methods [13] .', 'Automation occurs  in diﬀerent forms; from the most basic of tasks to complicated ones [13] , such as removing duplicate articles, prioritizing articles  for screening, and extracting data from tables and ﬁgures [ 14 , 15 ].', 'Research on diﬀerent approaches for automating systematic  reviews via technologies such as machine learning, text mining, and natural language processing exists [12] .', 'Text mining is the  process of discovering knowledge and structure from unstructured data [16] , while Natural Language Processing (NLP) supports  human analysts to carry out various linguistic analytical tasks on textual documents [17] , such as identifying potential keywords in  systematic literature reviews [18–20] .', 'Using NLP to extract information from text automatically leads to decreased labor of manual  extraction from a large volume of text material and saves time [21] .', 'However, automation in SR has focused chieﬂy on extracting data or results after a literature search, while methods or strategies  to ﬁnd or assemble all relevant evidence, including developing a search strategy, have received little attention [22] .', 'According to  [22] , search strategies for SR should be able to return all the studies relevant to the review (‘recall’) without retrieving irrelevant  studies (‘precision’).', 'Unfortunately, not all ﬁelds of study have a structured or standardized ontology for search strategy development.', ' The ﬁeld of public health has institutionalized support and standardized ontology (i.e., Medical Subject Headers, or MeSH) for search  strategy development [23] .', 'However, ecology or environmental sciences, generally, does not have standardized ontologies.', 'Thus,  researchers tend to use broad, non ‐speciﬁc keywords in their search (Pullin & Stewart, 2006), leading to low precision of search  results (0.473%; [2] ).', 'With low precision, more time and cost are spent on screening articles.', 'Thus, enhanced standardization in search  strategy development is critical to improving the speciﬁcity, objectivity, and reproducibility of SRs [24] .', 'Two primary approaches  for automating search strategy development are citation networks and text mining [22] ; both use a set of predetermined articles that  researchers deem relevant to the review.', 'Thus, both approaches require researchers to select a starting set of articles with which they ', '\\x0cA.E. Kwabena, O.-B.', 'Wiafe, B.-D.', 'John et al.', 'MethodsX 10 (2023) 101935 ', 'Fig.', 'Use case diagram for Ananse.', 'are already familiar.', 'This predisposes citation networks and text mining towards familiar articles.', 'Although this approach has high  precision, it has a low recall, and the risk of selection, citation, and publication bias is increased as the initial set of articles inﬂuences  what is eventually retrieved [ 25 , 22 , 26 ].', 'In this research, we mediate the problems associated with search strategy development in systematic literature reviews by devel-  oping a method that uses NLP and keyword co-occurrence networks to identify potential keywords to support SR.', 'We adapted the  search terms strategy for naïve search proposed by [22] written in R.', 'To facilitate reproducibility and transparency; we created the  python package dubbed ‘Ananse’ (a Ghanaian vernacular translated as a spider) to aid the implementation of the method in a user-  friendly format.', 'The software and documentation are publicly available via Github [27] and PyPI [28–30] , respectively.', 'We tested  our approach by applying it to selecting keywords for a systematic literature review of cumulative eﬀect assessment of disturbance  on forest ecosystems (see [30] ).', 'The remainder of the study is structured as follows.', 'Materials and methods are presented in section 2 , where the process ﬂow  of Ananse in ﬁnding search terms are described.', 'Using Ananse to perform a search tailored to a SR of cumulative eﬀect studies is  described in section 3 .', 'In section 4 , we discuss the outcomes of using Ananse to perform cumulative eﬀect search terms [30] and  compare our results with other related works.', 'Finally, in section 5 , we draw conclusions based on our ﬁndings and forecast future  work.', 'Methods details ', 'We developed a Python package to partially automate search term selection and write search strategies for SRs. We refer to this  Python package as Ananse (a Ghanaian vernacular translated as a spider).', 'We adapted the search strategy for black-backed woodpecker  occupancy of post-ﬁre forest systems ( [22] and [31] ) written in R.', 'Our search term selection strategy focuses on cumulative eﬀect and  seeks to create an open-source search software in Python.', 'Software design ', 'Software design describes the structure of the software to be implemented, the data models used by the system, the interfaces,  and, sometimes, the algorithms used [32] .', 'Requirements usually precede the design.', 'We present the following design considerations  during the creation of Ananse: functional requirements, use case diagram, and data ﬂow diagram.', 'We do not intend to oﬀer a technical  software engineering perspective but to guide the user to appreciate the design concepts which gave birth to Ananse.', 'Functional requirements ', 'The functional requirements for a software system describe what the system should do [ 33 , 34 ].', 'We considered the SR process ', 'from the NLP perspective and speciﬁed the requirements for Ananse.', 'Ananse is able to: ', 'Import results of a naïve search from a literature database such as JSTOR, Web of Science, and Scopus just to mention a few.', 'Deduplicate combined search results.', '\\x0cA.E. Kwabena, O.-B.', 'Wiafe, B.-D.', 'John et al.', 'MethodsX 10 (2023) 101935 ', 'Fig.', 'Ananse process ﬂow ', 'Extract terms using Rapid Automatic Keyword Extraction (RAKE) algorithm  4.', 'Create document term matrix.', 'Convert document term matrix into data frames.', 'Create document network from data frames.', 'Generate node strength and ﬁnal cut-oﬀ.', 'Generate keywords.', 'These eight requirements were used to formulate a use case diagram.', 'Use case diagram ', 'Use cases are documented using a high-level use case diagram.', 'The set of use cases represents all of the possible interactions  described in the system requirements.', 'Actors in the process, who may be human or other systems, are represented as stick ﬁgures.', ' Each class of interaction is represented as a named ellipse.', 'Lines link the actors with the interaction; arrowheads show how the  interaction is initiated.', 'Figure 1 is the use case diagram for Ananse.', 'A researcher performs naïve a search from a journal database platform such as Web ', 'of Science, Scopus, or JSTOR.', 'Flow diagram ', 'Figure 2 shows the process ﬂow used in implementing Ananse.', '\\x0cA.E. Kwabena, O.-B.', 'Wiafe, B.-D.', 'John et al.', 'MethodsX 10 (2023) 101935 ', 'Table 1  Search terms grouped under diﬀerent concept categories.', 'OR Economic Eﬀects ∗ ', ') OR (Human health ∗ ', 'OR Cumulative impart ∗ ', 'OR forest management ∗ ', 'OR Systematic approach ∗ ', 'OR forest community ∗  OR forest soil ∗ ', 'OR Environmental impact ∗  OR Strategic eﬀects ∗ ', 'OR forest ecosystem ∗  OR forest bird ∗  OR forest soil nutrients ∗ ', 'OR Cumulative disturbance ∗  ) OR (Strategic environmental assessment ∗ ', 'OR Cumulative environmental eﬀect ∗  )  OR Impact assessment ∗  ) OR (Social  OR Human Health Eﬀects ∗ ', 'Concept category 1: (Cumulative eﬀect ∗  OR (Environmental eﬀect ∗  eﬀect ∗  OR Economic eﬀects ∗  Regulatory drive OR Risk assessment ∗  Concept category 2: forest ∗  growth ∗  carbon ∗  vegetation ∗  population ∗  endangered species ∗  Concept category 3: Mining OR Minerals and metal OR Oil and gas OR Oil sands development OR Peat mining OR Storm (wind) OR  Pulp and paper industry OR Barriers OR Wildﬁre OR Planting OR Forest disease OR Forest health OR Forest pest OR Deforestation OR  Linear features OR Electricity generation OR Roads OR Power lines OR Seismic lines OR Urbanization OR Land reclamation  /restoration OR Global change OR Climate change OR Defoliation OR Insect outbreak OR water and wetlands OR Logging OR Wells  OR Flood OR Drought OR Hydro development ', 'OR forest cover OR forest  OR understory  OR  OR species composition ∗ ', 'OR Electricity generation OR forest stream ∗  OR water quality ∗ ', 'OR Indigenous people livelihoods ∗  OR community ∗ ', 'OR forest land ∗  OR forest biodiversity ∗ ', 'OR forest disturbance OR forest dynamics ∗ ', 'OR land use/cover conversion ∗ ', 'OR forest sustainability ∗ ', 'OR forest conservation ∗ ', 'OR forest structure ∗ ', 'OR water quantity ∗ ', 'OR forest policy ∗ ', 'OR silviculture ∗ ', 'OR ecosystem ∗ ', 'OR forest ', ') OR ', 'OR ', 'Fig.', 'Naïve search ﬁle and results from the three databases ', 'Naïve search is written and imported.', 'Results are assembled and deduplicated, followed by keyword extraction, creating a co- ', 'occurrence network, and identifying important nodes.', 'After getting results, the process can be initiated for other searches.', 'Software implementation and results ', 'Writing the naïve search and exporting the results ', 'When writing a naive search, the ﬁrst step is to clearly articulate the research question (Grames et al., 2020).', 'The na ї ve search  must be precise; otherwise, it will return several unrelated articles, weakening the subsequent keyword selection [22] .', 'The authors,  who are experts in the domain of cumulative eﬀect assessment, developed the initial search terms (76 search terms) under diﬀerent  concept categories to guide the identiﬁcation of studies for the naïve search.', 'We grouped the search terms into three concept categories  and combined them into a Boolean search (see Table 1 ).', 'Using the initial search terms of 76, we conducted a naïve literature search  in three sample databases: JSTOR, Scopus, and Web of Science.', 'These three databases were chosen to broaden the available pool of  search terms on the topic as their coverage diﬀers substantially [45] .', 'Importing naïve search results into Ananse ', 'Ananse is a package and is provoked through a ﬁle.', 'The naïve search results from Jstor, Scopus, and Web of Science databases  were exported as an ris ﬁle, csv ﬁle, and txt ﬁle, respectively; s ; Jstor with a .ris, Web of Science with a .csv ﬁle extension, and Scopus  with a .txt ﬁle extension.', 'Due to the diﬀerent formats in the exportation of results from the databases, this manual process takes  more time.', 'All these three ﬁles were fed into Ananse at the same time.', 'Using these ﬁles as input, Ananse merges all the diﬀerent ﬁle  formats into a single Pandas data frame.', 'The merging resulted in a csv ﬁle containing 129,407 articles.', 'Figure 3 shows the results of  the naïve search and the ﬁle “ananse_test.py ” that provokes Ananse to perform the search.', 'Assembling and deduplicating results ', 'Many articles indexed in multiple databases may pop up more than once searching for information, resulting in an overrepresen-  tation of terms.', 'The naïve search results were assembled and deduplicated to prevent over-representation.', 'Provided that the path  to the directory of search results is given, the import_naive_results function in Ananse automatically ﬁnds each ﬁle’s database and ﬁle  type, selects analogous columns, and joins them to form a single dataset.', 'This function imports the search results from a speciﬁed  path.', 'If the parameters clean_dataset and save_dataset are set to TRUE , the function deduplicates search results after importing and  saves the full search results to a csv ﬁle.', 'The parameter save_directory contains the path to a directory where search results will be  saved.', 'If save_dataset is set to TRUE while the parameter save_directory is set to the directory of choice, the merged ﬁle is saved to that  directory path containing the naive search results ﬁles.', 'After the results are obtained, a pandas data frame consisting of assembled  search results is returned.', 'After the merging, Ananse performs deduplication based on the article titles and abstracts and returns  diﬀerent articles.', 'In this instance, Ananse removes the exact title duplicates; titles that are over 95% similar or abstract that are more ', '\\x0cA.E. Kwabena, O.-B.', 'Wiafe, B.-D.', 'John et al.', 'MethodsX 10 (2023) 101935 ', 'Fig.', 'Screenshot of deduplicated ﬁles in csv format ', 'than 90% similar are removed.', 'The user can change these similarity levels.', 'Ananse returned 6,786 distinct articles out of the 7,809  articles fed into it and created a csv ﬁle, a screenshot of it is as shown in Fig.', '4 (the content of the csv ﬁle is available in the appendix).', ' Ananse automatically corrected and classiﬁed 100% of the 1023 duplicate articles identiﬁed by manual deduplication.', 'Extracting and identifying keywords ', 'Ananse uses the Rapid Automatic Keyword Extraction (RAKE) [35] , a keyword extraction method, to extract potential keywords  from the titles, keywords and abstracts of articles in the deduplicated dataset.', 'The RAKE is designed to identify keywords in sci-  entiﬁc literature by selecting strings of words uninterrupted using a list of stopwords (6 + ) and phrase delimiters (punctuation) to  detect the most relevant words or phrases in a piece of text [36] .', 'The function extract_terms call the RAKE algorithm and eliminates  keywords that only appear in a single article and excludes phrases with only one word from the list of potential keywords result-  ing in a more precise search.', 'Ananse then combines the author- and database-tagged keywords with the search terms.', 'The author  and database tagged keywords are combined as dictionary objects created with extract_terms to deﬁne all possible keywords.', 'All the  possible keywords are then passed to a function create_dtm for function wrapping, which generates a document-feature matrix using  the potential keywords as features and the combined titles, abstracts, and keywords of each article (also referred to as noted) as the  documents.', 'Co-occurrence network ', 'The selection of keywords using the frequency of occurrence can be a good indicator of the relevance of a word/term to a search  strategy.', 'However, we moved beyond this and generated a keyword co ‐occurrence network.', 'The co-occurrence network creates and  measures each term’s importance and inﬂuence in relation to the topic being reviewed [37] .', 'Using the document matrix containing  the potential keywords, we generated a keyword co-occurrence network.', 'Each keyword is represented by a point referred to as the  node, and an edge also represents a link between the keywords.', 'Each node represents a potential search term, and the edges are co-  occurrences of two terms in a study’s title, abstract, or tagged keywords [37] .', 'In Ananse , the co-occurrence network is implemented  with the function create_network, which measures the importance of each term in relation to the selected topic being reviewed.', ' The function get_centrality is used to evaluate the node importance of a graph and returns a dictionary containing nodes with their  importance.', 'Figure 5 shows a co-occurrence network with important keywords closely grouped.', 'The dense region shows keywords that are ', 'closely related.', 'Identifying important nodes using a full network ', 'Important  nodes  represent  keywords  to  be  used  to  generate  ﬁnal  search  terms.', ' Two  methods  to  identify  impor-  tant  nodes  were  explored  in  Ananse :  ﬁtting  a  spline  model  to  the  node  importance  to  select  tipping  points  and  cu-  mulative  approach,  which  ﬁnds  the  minimum  number  of  nodes  to  capture  a  large  percentage  of  the  total  impor-  tance  of  the  network.', ' One  can  decide  which  method  to  use  depending  on  the  distribution  and  preference.', ' In  choos-  ing  a  method,  the  ﬁrst  thing  to  do  is  to  look  at  the  distribution  of  node  importance.', ' In  Ananse,  the  distribu-  tion  was  plotted  with  the  function  plot_degree_distribution,  plot_rank_degree_distribution,  or  plot_degree_histogram  as  shown  in  Fig.', 'A spline model for ﬁnding cut-oﬀ is an appropriate method to identify the cut-oﬀ threshold for keyword importance if the rank  distribution plot has a lot of weak nodes with a long tail.', 'On the other hand, the cumulative approach is more appropriate when  there are no clear breaks in the data.', 'In Ananse, the ﬁnd_cutoﬀ function ﬁnds the cut-oﬀ for a graph network using either cumulative  or spline method of cutting the degree distribution, as shown in Fig.', 'The reduce_graph function then generates a graph consisting  of only important nodes, after which the get_keyword function extracts the keywords from the reduced network.', 'Ananse uses the node strength to generate relevant keywords from which the experts can now select their ﬁnal keywords.', 'In  this research, Ananse generated 4,596 keywords.', 'A screenshot of it is shown in Fig.', '8 (the content of the csv ﬁle is available in the  appendix).', 'Afterward, the researchers manually reviewed each word or phrase using their expert knowledge to arrive at the ﬁnal  keywords.', 'The ﬁnal list of search terms (listed as search strings) was grouped under three concepts, as shown in Table 2 .', 'These concepts ', '(and terminology) are cumulative eﬀects, forests and forest ecosystems, and types and forms related to forest disturbance [30] .', '\\x0cA.E. Kwabena, O.-B.', 'Wiafe, B.-D.', 'John et al.', 'MethodsX 10 (2023) 101935 ', 'Fig.', 'Co-occurrence Network from the case study ', 'Discussion ', 'Evidence synthesis has become an essential feature of the current academic landscape, although a lack of transparency often  hampers the process.', 'This research reports on the methods used to select search terms that form the building block for performing  evidence synthesis and oﬀers a transparent approach to understand underlying assumptions.', 'In systematic reviews, the selection of  key search terms is considered the basic building block for the successful assemblage of knowledge in a particular ﬁeld.', 'However, this  process is often left to researchers’ discretion, leaving room for biases and a subjective selection process, aﬀecting the outcomes of  eﬀective evidence synthesis.', 'In this research, we designed and implemented a partially automated keyword search software package  using Python for SR to enhance eﬃciency, maximize transparency and comprehensiveness while minimizing subjectivity and bias.', ' Dubbed Ananse , our tool provides an eﬃcient and standardized method for developing search strategies using NLP and co-occurrence  networks to identify relevant search terms.', 'Our approach combines expert knowledge with a quasi ‐automated method which enhances search recall.', 'This is very important  for ﬁelds such as ecology, where non ‐standardized or nuanced terminology or a lack of formal ontologies exist for conducting SRs  [22] .', 'Most importantly, Ananse signiﬁcantly reduces the time required to conduct a SR by decreasing time spent on search strategy  development and tedious tasks like assembling and deduplication.', 'Compared with the manual process of assembling results, Ananse  reduced by more than half the time required to assembly results.', 'Similarly, while it took two of the co-authors two days of full-time  work to remove duplicates, Ananse removed the duplicates eﬃciently in about a minute or less and achieved 100% accuracy.', 'With  the reduction in time needed to develop a search strategy and assemble and deduplicate the results, our approach makes extensive  systematic reviews and meta ‐analyses more eﬃcient and eﬀective compared with conventional approaches.', 'Our research contributes  to the emergence and application of an ever-growing set of tools and software that can be used to facilitate transparent, reproducible  reviews and develop reproducible synthesis workﬂows such as metaDigitise [38] , litsearchr [22] in R, and revtools [39] .', 'These eﬀorts  should help facilitate the reproducibility of ecological reviews, enhance transparency, and improve the rigor of evidence used to guide  policy decisions [40] .', 'In its current implementation , Ananse, a Python package, contains a suite of functions to improve the eﬃciency of keywords  selection for systematic reviews.', 'For instance, by automatically deduplicating and assembling results from separate databases, Ananse  provides a systematic approach to facilitate knowledge synthesis through SR.', 'Also, apart from generating keywords, it can act as  middleware or a data converter for integrating multiple datasets into a database.', 'Done manually, this is a time ‐intensive process ', '\\x0cA.E. Kwabena, O.-B.', 'Wiafe, B.-D.', 'John et al.', 'MethodsX 10 (2023) 101935 ', 'Fig.', 'Degree Histogram of degree and counts ', 'because platforms and databases export results in diﬀerent formats [2] .', 'Furthermore, we used the agile method of software engineering  with open-source software development, thereby making Ananse easily customizable and improved upon as researchers use it beyond  the application to cumulative eﬀects assessments.', 'Currently, Ananse has a popularity of 131 downloads per week on the Python  Package Index ( https://snyk.io/advisor/python/ananse ).', 'Ananse contributes to the development of open-source software systems  needed to speed up systematic review.', 'In its current state, Ananse provides a means to merge and deduplicate keywords for experts  programmatically.', 'By its design and implementation, Ananse allows researchers to modify their requirements without creating new  software.', 'Even though Ananse has been used for a cumulative eﬀect use case [30] , it is general-purpose software for a systematic  review of any kind.', 'It can be applied broadly in ecology and evolutionary biology as well as other ﬁelds.', 'Conclusion ', 'Compared to conventional approaches for developing keywords for systematic review, our method is far eﬀective and eﬃcient by  signiﬁcantly reducing the time and resources needed to develop search strategies to conduct systematic reviews.', 'Ananse substantially  reduces the time spent on the systematic review by automating time-consuming tasks such as assembling and deduplicating large  search results.', 'Ananse saves time and enhances eﬀective keyword generation compared to traditional methods by automating the  tedious and bias-prone aspect of systematic review tasks.', 'Therefore, Ananse presents an approach to performing large systematic  reviews within a short period of time.', 'Our results can be used as a starting point to frame future studies according to well-deﬁned terminology.', 'Future research would  enhance the front-end of Ananse through a user-friendly graphical interface.', 'Currently, Ananse allows one user per time; this func-  tionality can be improved by making Ananse a server-type software with capabilities to permit concurrent and multi-user interaction.', ' The requirements would be modiﬁed as we get feedback from the research community.', '\\x0cA.E. Kwabena, O.-B.', 'Wiafe, B.-D.', 'John et al.', 'MethodsX 10 (2023) 101935 ', 'Fig.', '(Ranked Node Strength with cut-oﬀ points) ', 'Fig.', 'A section of relevant keywords.', 'Software, data, and documentation availability ', 'The source of this software is publicly available via Github [41] and also via PyPI [42] .', 'Documentation is accessible via [43] and ', 'Acknowledgements ', 'The CFS Cumulative Eﬀects Program supported this work.', 'We want to thank Sonja Kosuta, Tracey Cook, and Danny Galarneau ', 'from Natural Resources Canada for their instrumental roles in moving the study forward.', '\\x0cA.E. Kwabena, O.-B.', 'Wiafe, B.-D.', 'John et al.', 'MethodsX 10 (2023) 101935 ', 'Table 2  Final list of search terms.', 'Concept A:  Cumulative eﬀets terminologies ', 'Concept B:  Resource development/disturbance ', 'Concept C:  Forest landscape dynamics ', 'Cumulative eﬀect  Cumulative impact  Environmental eﬀect  Environmental impact  Cumulative disturbance  Impact assessment  Cumulative environmental eﬀect  Social eﬀects  Economic eﬀects  Strategic environmental assessment  Risk assessment  Systematic approach  Human health  Human Health Eﬀects  Regulatory drive ', 'Mining  Minerals and metal  Oil and gas  Oil sands development  Peat mining  Storm (wind)  Pulp and paper industry  Barriers  Wildﬁre  Planting  Forest disease  Forest health  Forest pest  Deforestation  Linear features  Electricity generation  Roads  Power lines  Seismic lines  Urbanization  Land reclamation /restoration  Global change  Climate change  Defoliation  Insect outbreak  water and wetlands  Logging  Wells  Flood  Drought  Hydro development  Habitat fragmentation  Landscape fragmentation  Species invasion  Urban expansion  Habitat alteration  Loss of biological diversity  Soil acidiﬁcation  Forest harvesting  Air Pollution  Water pollution ', 'forest  forest ecosystem  forest management  forest disturbance  forest dynamics  forest growth  understory vegetation  forest community  forest bird  forest land  Indigenous people livelihoods  forest policy  forest sustainability  forest cover  forest carbon  landscape  forest stream  silviculture  ecosystem  population  community  land cover conversion  water quality  water quantity  forest soil  forest soil nutrients  forest biodiversity  forest conservation  forest structure  species composition  endangered species  forest habitat  wildlife  soil compaction  soil porosity  soil quality  functional traits  Forest soil biodiversity ', 'Declaration of Competing Interest ', 'The authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to ', 'inﬂuence the work reported in this paper.', 'Funding ', 'This research was funded by Natural Resources Canada (Canadian Forest Service, Forest Ecosystem Integrity and Cumulative  Eﬀects Programs).', 'Natural Resources Canada (Canadian Forest Service, Forest Ecosystem Integrity, and Cumulative Eﬀects Programs)  did not play any role in preparing this manuscript.', 'References ', '[1] CochraneEvidence Synthesis - What is it and why do we need it?, Cochrane, 2021 https://www.cochrane.org/news/evidence-synthesis-what-it-and-why-do-we-need-it ', '(accessed May 06, 2021) .', '[2] N.R. Haddaway, M.J. Westgate, Predicting the time needed for environmental systematic reviews and systematic maps, Conservation Biology 33 (2019) 2 ', 'Blackwell Publishing Inc., pp.', '434–443, Apr.', '01, 2019, doi: 10.1111/cobi.13231 .', '[3] W.J. Sutherland, C.F. Wordley, A fresh approach to evidence synthesis, Nature 558 (2018) 364–366, doi: 10.1038/d41586-018-05472-8 .', ' [4] J.', 'Lind, A Treatise of the Scurvy, in Three Parts, Cambridge University Press, 2014 .', ' [5] Peri či ć, T.P. and Tanveer, S.', '“Why systematic reviews matter.', '” https://www.elsevier.com/connect/authors-update/why-systematic-reviews-matter (ac- ', 'cessed May 06, 2021).', '[6] O.', 'Berger-Tal, A.L. Greggor, B.', 'Macura, C.A. Adams, A.', 'Blumenthal, A.', 'Bouskila, .', 'D.T. Blumstein, Systematic reviews and maps as tools for applying behavioral ', 'ecology to management and policy, Behavioral Ecology 30 (1) (2019) 1–8 .', '\\x0cA.E. Kwabena, O.-B.', 'Wiafe, B.-D.', 'John et al.', 'MethodsX 10 (2023) 101935 ', '[7] W.', 'Mengist, T.', 'Soromessa, G.', 'Legese, Method for conducting systematic literature review and meta-analysis for environmental science research, MethodsX 7 ', '[8] Curtin  University  (2021)  “What  is  a  systematic  review?', ' -  Systematic  Reviews  in  the  Health  Sciences  -  LibGuides  at  Curtin  University.', 'https://libguides.library.curtin.edu.au/systematic-reviews (accessed May 06, 2021).', '[9] I.J. Marshall, B.C. Wallace, Toward systematic review automation: a practical guide to using machine learning tools in research synthesis, Systematic reviews 8 ', '[10] B.', 'Nussbaumer-Streit, M.', 'Ellen, I.', 'Klerings, R.', 'Sfetcu, N.', 'Riva, M.', 'Mahmi ć-Kaknjo, .', 'G.', 'Gartlehner, Resource use during systematic review production varies ', 'widely: a scoping review, Journal of Clinical Epidemiology 139 (2021) 287–296 .', '[11] K.', 'Bullers, A.M. Howard, A.', 'Hanson, W.D. Kearns, J.J. Orriola, R.L. Polo, K.A. Sakmar, It takes longer than you think: librarian time spent on systematic review ', 'tasks, Journal of the Medical Library Association : JMLA 106 (2) (2018) 198 .', '[12] O’Mara-Eves, A., Thomas, J., McNaught, J., Miwa, M., & Ananiadou, S.', 'Using text mining for study identiﬁcation in systematic reviews: a systematic ', 'review of current approaches.', 'Systematic reviews , 4(1), 1-22.', 'doi: 10.1186/2046-4053-4-5 .', '[13] G.', 'Tsafnat, P.', 'Glasziou, M.K. Choong, A.', 'Dunn, F.', 'Galgani, E.', 'Coiera, Systematic review automation technologies, Systematic reviews 3 (1) (2014) 1–15 .', ' [14] J.', 'Rathbone, T.', 'Hoﬀmann, P.', 'Glasziou, Faster title and abstract screening?', 'Evaluating Abstrackr, a semi-automated online screening program for systematic ', 'reviewers, Systematic reviews 4 (1) (2015) 1–7 .', '[15] I.', 'Shemilt, A.', 'Simon, G.J. Hollands, T.M. Marteau, D.', 'Ogilvie, A.', 'O’Mara-Eves, .', 'J.', 'Thomas, Pinpointing needles in giant haystacks: use of text mining to reduce ', 'impractical screening workload in extremely large scoping reviews, Research Synthesis Methods 5 (1) (2014) 31–49 .', '[16] S.', 'Ananiadou, J.', 'McNaught, Text mining for biology and biomedicine, 2006 Boston, MA .', ' [17] L.', 'Zhao, W.', 'Alhoshan, A.', 'Ferrari, K.J. Letsholo, M.', 'Ajagbe, E.V. Chioasca, R.T. Batista-Navarro, Natural Language Processing (NLP) for Requirements Engineering ', '(RE): A Systematic Mapping Study, ACM Computing Surveys (2020) .', '[18] Basyal, G.', 'P., Rimal, B.', 'P., & Zeng, D.', 'A Systematic Review of Natural Language Processing for Knowledge Management in Healthcare.', 'arXiv preprint ', 'arXiv:2007.09134 ', '[19] A.', 'Turchin, L.F. Florez Builes, Using Natural Language Processing to Measure and Improve Quality of Diabetes Care: A Systematic Review, Journal of Diabetes ', 'Science and Technology 15 (3) (2021) 553–560 .', '[20] J.', 'Wang, H.', 'Deng, B.', 'Liu, A.', 'Hu, J.', 'Liang, L.', 'Fan, .', 'J.', 'Lei, Systematic evaluation of research progress on natural language processing in medicine over the past ', '20 years: Bibliometric study on PubMed, Journal of medical Internet research 22 (1) (2020) e16816 .', '[21] M.', 'Montazeri, A.', 'Afraz, R.M. Farimani, F.', 'Ghasemian, Natural Language Processing Systems for Diagnosing and Determining Level of Lung Cancer: A Systematic ', 'Review, Frontiers in Health Informatics 10 (1) (2021) 68 .', '[22] E.M. Grames, A.N. Stillman, M.W. Tingley, C.S. Elphick, An automated approach to identifying search terms for systematic reviews using keyword co-occurrence ', 'networks, Methods in Ecology and Evolution 10 (10) (2019) 1645–1654 .', '[23] Wichor M.', 'Bramer, Gerdien B.', 'De Jonge, Melissa L.', 'Rethlefsen, Frans Mast, Jos Kleijnen, A systematic approach to searching: an eﬃcient and complete method ', 'to develop literature searches, Journal of the Medical Library Association: JMLA 106 (4) (2018) 531 .', '[24] C.', 'Stansﬁeld, A.', 'O’Mara-Eves, J.', 'Thomas, Text mining for search term development in systematic reviewing: A discussion of some methods and challenges, ', 'Research synthesis methods 8 (3) (2017) 355–365 .', '[25] C.W. Belter, Citation analysis as a literature search method for systematic reviews, Journal of the Association for Information Science and Technology 67 (11) ', '[26] Sarol, M.', 'J., Liu, L., & Schneider, J.', '(2018, January).', 'Testing a citation and text-based framework for retrieving publications for literature reviews.', 'In BIR@ECIR .', ' [27] Antwi, E, Owusu-Banahene W., Boakye-Danquah J., Asare, B.', 'A and Frimpong-Boateng A.', 'F (2020a), Ananse: https://github.com/baasare/ananse  [28] Antwi, E, Owusu-Banahene W., Boakye-Danquah J., Asare, B.', 'A and Frimpong-Boateng A.', 'F (2020b), Ananse 1.1.2.: https://pypi.org/project/ananse  [29] Antwi,  E,  Owusu-Banahene  W.,  Boakye-Danquah  J.,  Asare,  B.', ' A  and  Frimpong-Boateng  A.', ' F  (2020c),  Ananse  Documentation: ', 'https://ananse.readthedocs.io/en/latest/ ', '[30] E.K. Antwi, J.', 'Boakye-Danquah, W.', 'Owusu-Banahene, K.', 'Webster, A.', 'Dabros, P.', 'Wiebe, .', 'A.K. Sarfo, A Global review of cumulative eﬀects assessments of ', 'disturbances on forest ecosystems, Journal of Environmental Management 317 (2022) 115277 .', '[31] Grames, Stillman, Introduction to litsearchr with an example of writing a systematic review search strategy for black-backed woodpecker occupancy of post-ﬁre ', 'forest systems, Elizagrames.github.io (2020) 2020.', '[Online].', 'Available https://elizagrames.github.io/litsearchr/introduction_vignette_v010.html .', '[32] I.', 'Sommerville, M.', 'Fowler, K.', 'Beck, J.', 'Brant, W.', 'Opdyke, D.', 'Roberts, Edition: Software Engineering.', 'Instructor, 2019 .', ' [33] I.', 'Sommerville, Engineering Software Products, Pearson, 2020 .', ' [34] K.J. Stol, B.', 'Fitzgerald, The ABC of software engineering research, ACM Transactions on Software Engineering and Methodology (TOSEM) 27 (3) (2018) 1–51 .', ' [35] MonkeyLearn (2020) \"Keyword Extraction\".', 'Available: https://monkeylearn.com/keywordextraction/ .', ' [36] S.', 'Rose, D.', 'Engel, N.', 'Cramer, W.', 'Cowley, Automatic keyword extraction from individual documents, Text mining: applications and theory 1 (2010) 1–20 .', ' [37] P.C. Lee, H.N. Su, Investigating the structure of regional innovation system research through keyword co-occurrence and social network analysis, Innovation 12 ', '[38] J.L. Pick, S.', 'Nakagawa, D.W. Noble, Reproducible, ﬂexible and high-throughput data extraction from primary literature: The metaDigitise r package, Methods ', 'in Ecology and Evolution 10 (3) (2019) 426–431 .', '[39] M.J. Westgate, revtools: An R package to support article screening for evidence synthesis, Research synthesis methods 10 (4) (2019) 606–614 .', ' [40] E.M. Grames, C.S. Elphick, Use of study design principles would increase the reproducibility of reviews in conservation biology, Biological Conservation 241 ', '[41] Antwi,  E,  Owusu-Banahene  W.,  Boakye-Danquah  J.,  Asare,  B.', ' A.', ' and  Frimpong-Boateng  AF.', ' (2020)(d),Ananse  documentation: ', 'https://baasare.github.io/ananse/_build/html/index.html ', '[42] Cornell  University  (2021).', ' “Types  of  Evidence  Synthesis  -  A  Guide  to  Evidence  Synthesis  -  LibGuides  at  Cornell  University.', 'https://guides.library.cornell.edu/evidence-synthesis/types (accessed May 06, 2021).', '[43] E.', 'Hausner, S.', 'Waﬀenschmidt, T.', 'Kaiser, M.', 'Simon, Routine development of objectively derived search strategies, Systematic reviews 1 (1) (2012) 1–10 .', ' [44] Hsin-ning Su, Pei-Chun Lee, Mapping knowledge structure by keyword co-occurrence: A ﬁrst look at journal papers in Technology Foresight, Scientometrics 85 ', '[45] P.', 'Mongeon, A.', 'Paul-Hus, The journal coverage of Web of Science and Scopus: a comparative analysis, Scientometrics 106 (1) (2016) 213–228 .']\n"
     ]
    }
   ],
   "source": [
    "for doc in documents[-3:-2]:\n",
    "    print('Title: '+doc.title, \n",
    "          '\\n\\tFilepath: '+doc.file)\n",
    "    sentences = doc.split_sentences()\n",
    "    print(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Text from PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    def __init__(self, sentence, file):\n",
    "        self.file = file\n",
    "\n",
    "        self.tokens = None\n",
    "        self.inventory = None\n",
    "        self.contains_noun = None\n",
    "        self.contains_verb = None\n",
    "        self.contains_cid = None\n",
    "        self.valid = None\n",
    "\n",
    "        #replace tailing digits on words. those digits are usually footnotes\n",
    "        self.sentence = re.sub(r'[A-Za-z]\\d+\\b', '', sentence)\n",
    "\n",
    "         #corp sentence to beginning based on first alphabtic character\n",
    "        for i, char in enumerate(self.sentence):\n",
    "            if char.isalpha() and re.match(r'[A-Z]',char):\n",
    "                self.sentence = self.sentence[i:]\n",
    "                break\n",
    "        \n",
    "\n",
    "\n",
    "    def tokenize(self):\n",
    "        self.tokens = [(word.text, word.pos_) for word in nlp(self.sentence)]\n",
    "    \n",
    "    def count_tokens(self):\n",
    "        if self.tokens is None:\n",
    "            self.tokenize()\n",
    "\n",
    "        inventory = {}\n",
    "        for _, value in self.tokens:\n",
    "            inventory[value] = inventory.get(value, 0) + 1\n",
    "    \n",
    "        self.inventory = inventory\n",
    "    \n",
    "    def check_validity(self):\n",
    "        if self.inventory is None:\n",
    "            self.count_tokens()\n",
    "\n",
    "        word_types = self.inventory.keys()\n",
    "\n",
    "        if 'NOUN' in word_types:\n",
    "            self.contains_noun = True\n",
    "        else:\n",
    "            self.contains_verb = False\n",
    "\n",
    "        if 'VERB' in word_types:\n",
    "            self.contains_verb = True\n",
    "        else:\n",
    "            self.contains_verb = False\n",
    "\n",
    "        if re.match(r'\\(cid:\\d{1,4}\\)', self.sentence):\n",
    "            self.contains_cid = True\n",
    "        \n",
    "        if self.contains_noun and self.contains_verb:\n",
    "            self.valid = True\n",
    "        else:\n",
    "            self.valid = False\n",
    "    \n",
    "    def summarize(self, show_token_details=False):\n",
    "        print(f'The origin file is: {self.file}')\n",
    "        print(f'The sentence is:\\n{self.sentence}')\n",
    "        print(f'The inventory holds:\\n{self.inventory}')\n",
    "        if show_token_details:\n",
    "            print(f'The token details are:\\n{self.tokens}')\n",
    "\n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sentences = []\n",
    "\n",
    "for sen in sentences:\n",
    "\n",
    "    Sen = Sentence(sen, documents[0])\n",
    "    Sen.check_validity()\n",
    "    if Sen.valid:\n",
    "        valid_sentences.append(Sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 An automated method for developing search strategies for  systematic review using Natural Language Processing (NLP) \n",
      "2 Method name:  Search Strategy  Search Terms  Data Deduplication  Software Implementation  Evidence Synthesis  Systematic Literature Review  Text mining and keyword co ‐occurrence  networks to identify the most important terms  for a review \n",
      "3 The design and implementation of systematic reviews and meta-analyses are often hampered by  high ﬁnancial costs, signiﬁcant time commitment, and biases due to researchers’ familiarity with  studies.\n",
      "4 We proposed and implemented a fast and standardized method for search term selection  using Natural Language Processing (NLP) and co-occurrence networks to identify relevant search  terms to reduce biases in conducting systematic reviews and meta-analyses.\n",
      "5 The method was implemented using Python packaged dubbed Ananse, which is benchmarked  on the search terms strategy for naïve search proposed by Grames et al.\n",
      "6 Ananse was applied to a case example towards ﬁnding search terms to implement a systematic  literature review on cumulative eﬀect studies on forest ecosystems.\n",
      "7 The software automatically corrected and classiﬁed 100% of the duplicate articles identiﬁed  by manual deduplication.\n",
      "8 Ananse was applied to the cumulative eﬀects assessment case study,  but it can serve as a general-purpose, open-source software system that can support extensive  systematic reviews within a relatively short period with reduced biases.\n",
      "9 Besides generating keywords, Ananse can act as middleware or a data converter for integrating \n",
      "10 Corresponding Author.\n",
      "11 Received 15 November 2021; Accepted 18 November 2022  Available online 23 November 2022  2215-0161/Crown Copyright © 2022 Published by Elsevier B.V. This is an open access article under the CC BY license  ( http://creativecommons.org/licenses/by/4.0/ ) \n",
      "12 Environmental Science  Evidence synthesis in environmental and biological sciences  Text mining and keyword co ‐occurrence networks to identify the most important terms for a  review  Grames, E.\n",
      "13 An automated  approach to identifying search terms for systematic reviews using keyword  co ‐occurrence networks.\n",
      "14 Historically, summaries of scientiﬁc evidence have helped discover patterns of phenomena, develop theories or concepts, and  inform practice.\n",
      "15 Although common with editors and readers alike, this approach is less rigorous since evidence summarized this way  is less likely to answer speciﬁc clinical questions and more likely to contain literature selected by the authors and recommendations  prejudiced strongly by opinion.\n",
      "16 With exponential growth in scientiﬁc literature, the search for a structured and eﬀective evidence  synthesis has become a critical scientiﬁc endeavor.\n",
      "17 Evidence synthesis involves combining information from multiple studies or  research that have investigated the same or similar issue to come to a conclusive understanding of a speciﬁc topic [1] .\n",
      "18 It often  involves summarizing trends, identifying emerging questions, and clarifying disagreements and conﬂicting results [ 2 , 3 ].\n",
      "19 Since 1753 when James Lind published the ﬁrst evidence synthesis to provide a concise and unbiased summary of evidence  on scurvy, improvement in the state of evidence synthesis has grown [ 4 , 5 ].\n",
      "20 In the past two decades, advances in computer-aided  technology have enabled the growth and development of various forms of evidence synthesis.\n",
      "21 The two central techniques known  to have originated from the medical sciences and are commonly used today to synthesize evidence are systematic reviews (SRs) – which search available literature for evidence that addresses the research question, - and meta-analyses – which quantitatively assess  statistical evidence found through systematic reviews [5] .\n",
      "22 Evolutionary and behavioral ecologists started adopting meta-analyses  in the mid-1990s and became fully embraced since 2010 [6] .\n",
      "23 Meta-analysis has since become the gold standard for combining  information from multiple studies across disciplines.\n",
      "24 However, a good meta-analysis is dependent on a good sampling of the core  universe of studies, thus requiring a careful and comprehensive SR.\n",
      "25 A SR involves the review of an articulated research question  using systematic and testable methods to help to identify, select and evaluate all pertinent research [7] , and collect and analyze data  from the studies that are included in the review [8] .\n",
      "26 An excellent SR assembles and presents an impartial and objective summary of  ﬁndings, assesses all results for inclusion/exclusion and quality, and minimizes bias at all stages of the process [7] .\n",
      "27 However, the process of evidence synthesis is very tedious and often involves experienced methodologists and disciplinary experts  combing through all relevant studies, both published and unpublished, through a guided methodological process.\n",
      "28 As such, it tends  to be costly and tedious as it can take months, or even years, to complete, making it practically challenging [9] .\n",
      "29 According to  some estimates, conducting a SR can take up to 2 years to complete.\n",
      "30 SR with  meta-analyses ranges from 216 to 2,518 hours.\n",
      "31 According to [11] , conducting an eﬀective systematic search requires an information  specialist’s expertise and time, who need an average aggregated time of 26.9 hours when developing a search strategy.\n",
      "32 Thus, the  design and implementation of evidence-based synthesis are hampered by high ﬁnancial costs [3] and signiﬁcant time commitment  [2] .\n",
      "33 To overcome time and resource constraints required to synthesize evidence, scholars have adopted automation of the laborious  tasks in SR [12] .\n",
      "34 Advances in computer-aided technology have helped automate aspects of the evidence synthesis process to improve  eﬃciency and cut costs and time while still maintaining the standards of conventional search methods [13] .\n",
      "35 Automation occurs  in diﬀerent forms; from the most basic of tasks to complicated ones [13] , such as removing duplicate articles, prioritizing articles  for screening, and extracting data from tables and ﬁgures [ 14 , 15 ].\n",
      "36 Research on diﬀerent approaches for automating systematic  reviews via technologies such as machine learning, text mining, and natural language processing exists [12] .\n",
      "37 Text mining is the  process of discovering knowledge and structure from unstructured data [16] , while Natural Language Processing (NLP) supports  human analysts to carry out various linguistic analytical tasks on textual documents [17] , such as identifying potential keywords in  systematic literature reviews [18–20] .\n",
      "38 Using NLP to extract information from text automatically leads to decreased labor of manual  extraction from a large volume of text material and saves time [21] .\n",
      "39 However, automation in SR has focused chieﬂy on extracting data or results after a literature search, while methods or strategies  to ﬁnd or assemble all relevant evidence, including developing a search strategy, have received little attention [22] .\n",
      "40 According to  [22] , search strategies for SR should be able to return all the studies relevant to the review (‘recall’) without retrieving irrelevant  studies (‘precision’).\n",
      "41 Unfortunately, not all ﬁelds of study have a structured or standardized ontology for search strategy development.\n",
      "42 The ﬁeld of public health has institutionalized support and standardized ontology (i.e., Medical Subject Headers, or MeSH) for search  strategy development [23] .\n",
      "43 However, ecology or environmental sciences, generally, does not have standardized ontologies.\n",
      "44 Thus,  researchers tend to use broad, non ‐speciﬁc keywords in their search (Pullin & Stewart, 2006), leading to low precision of search  results (0.473%; [2] ).\n",
      "45 With low precision, more time and cost are spent on screening articles.\n",
      "46 Thus, enhanced standardization in search  strategy development is critical to improving the speciﬁcity, objectivity, and reproducibility of SRs [24] .\n",
      "47 Two primary approaches  for automating search strategy development are citation networks and text mining [22] ; both use a set of predetermined articles that  researchers deem relevant to the review.\n",
      "48 Thus, both approaches require researchers to select a starting set of articles with which they \n",
      "49 This predisposes citation networks and text mining towards familiar articles.\n",
      "50 Although this approach has high  precision, it has a low recall, and the risk of selection, citation, and publication bias is increased as the initial set of articles inﬂuences  what is eventually retrieved [ 25 , 22 , 26 ].\n",
      "51 In this research, we mediate the problems associated with search strategy development in systematic literature reviews by devel-  oping a method that uses NLP and keyword co-occurrence networks to identify potential keywords to support SR.\n",
      "52 We adapted the  search terms strategy for naïve search proposed by [22] written in R.\n",
      "53 To facilitate reproducibility and transparency; we created the  python package dubbed ‘Ananse’ (a Ghanaian vernacular translated as a spider) to aid the implementation of the method in a user-  friendly format.\n",
      "54 We tested  our approach by applying it to selecting keywords for a systematic literature review of cumulative eﬀect assessment of disturbance  on forest ecosystems (see [30] ).\n",
      "55 The remainder of the study is structured as follows.\n",
      "56 Materials and methods are presented in section 2 , where the process ﬂow  of Ananse in ﬁnding search terms are described.\n",
      "57 Using Ananse to perform a search tailored to a SR of cumulative eﬀect studies is  described in section 3 .\n",
      "58 In section 4 , we discuss the outcomes of using Ananse to perform cumulative eﬀect search terms [30] and  compare our results with other related works.\n",
      "59 Finally, in section 5 , we draw conclusions based on our ﬁndings and forecast future  work.\n",
      "60 We developed a Python package to partially automate search term selection and write search strategies for SRs. We refer to this  Python package as Ananse (a Ghanaian vernacular translated as a spider).\n",
      "61 We adapted the search strategy for black-backed woodpecker  occupancy of post-ﬁre forest systems ( [22] and [31] ) written in R.\n",
      "62 Our search term selection strategy focuses on cumulative eﬀect and  seeks to create an open-source search software in Python.\n",
      "63 Software design describes the structure of the software to be implemented, the data models used by the system, the interfaces,  and, sometimes, the algorithms used [32] .\n",
      "64 Requirements usually precede the design.\n",
      "65 We present the following design considerations  during the creation of Ananse: functional requirements, use case diagram, and data ﬂow diagram.\n",
      "66 We do not intend to oﬀer a technical  software engineering perspective but to guide the user to appreciate the design concepts which gave birth to Ananse.\n",
      "67 The functional requirements for a software system describe what the system should do [ 33 , 34 ].\n",
      "68 We considered the SR process \n",
      "69 NLP perspective and speciﬁed the requirements for Ananse.\n",
      "70 Import results of a naïve search from a literature database such as JSTOR, Web of Science, and Scopus just to mention a few.\n",
      "71 Deduplicate combined search results.\n",
      "72 Extract terms using Rapid Automatic Keyword Extraction (RAKE) algorithm  4.\n",
      "73 Create document term matrix.\n",
      "74 Convert document term matrix into data frames.\n",
      "75 Create document network from data frames.\n",
      "76 Generate node strength and ﬁnal cut-oﬀ.\n",
      "77 Generate keywords.\n",
      "78 These eight requirements were used to formulate a use case diagram.\n",
      "79 Use cases are documented using a high-level use case diagram.\n",
      "80 The set of use cases represents all of the possible interactions  described in the system requirements.\n",
      "81 Actors in the process, who may be human or other systems, are represented as stick ﬁgures.\n",
      "82 Each class of interaction is represented as a named ellipse.\n",
      "83 Lines link the actors with the interaction; arrowheads show how the  interaction is initiated.\n",
      "84 A researcher performs naïve a search from a journal database platform such as Web \n",
      "85 Figure 2 shows the process ﬂow used in implementing Ananse.\n",
      "86 Table 1  Search terms grouped under diﬀerent concept categories.\n",
      "87 Naïve search ﬁle and results from the three databases \n",
      "88 Naïve search is written and imported.\n",
      "89 Results are assembled and deduplicated, followed by keyword extraction, creating a co- \n",
      "90 occurrence network, and identifying important nodes.\n",
      "91 After getting results, the process can be initiated for other searches.\n",
      "92 Writing the naïve search and exporting the results \n",
      "93 When writing a naive search, the ﬁrst step is to clearly articulate the research question (Grames et al., 2020).\n",
      "94 The na ї ve search  must be precise; otherwise, it will return several unrelated articles, weakening the subsequent keyword selection [22] .\n",
      "95 The authors,  who are experts in the domain of cumulative eﬀect assessment, developed the initial search terms (76 search terms) under diﬀerent  concept categories to guide the identiﬁcation of studies for the naïve search.\n",
      "96 We grouped the search terms into three concept categories  and combined them into a Boolean search (see Table 1 ).\n",
      "97 Using the initial search terms of 76, we conducted a naïve literature search  in three sample databases: JSTOR, Scopus, and Web of Science.\n",
      "98 These three databases were chosen to broaden the available pool of  search terms on the topic as their coverage diﬀers substantially [45] .\n",
      "99 Importing naïve search results into Ananse \n",
      "100 Ananse is a package and is provoked through a ﬁle.\n",
      "101 The naïve search results from Jstor, Scopus, and Web of Science databases  were exported as an ris ﬁle, csv ﬁle, and txt ﬁle, respectively; s ; Jstor with a .ris, Web of Science with a .csv ﬁle extension, and Scopus  with a .txt ﬁle extension.\n",
      "102 Due to the diﬀerent formats in the exportation of results from the databases, this manual process takes  more time.\n",
      "103 All these three ﬁles were fed into Ananse at the same time.\n",
      "104 Using these ﬁles as input, Ananse merges all the diﬀerent ﬁle  formats into a single Pandas data frame.\n",
      "105 The merging resulted in a csv ﬁle containing 129,407 articles.\n",
      "106 Figure 3 shows the results of  the naïve search and the ﬁle “ananse_test.py ” that provokes Ananse to perform the search.\n",
      "107 Assembling and deduplicating results \n",
      "108 Many articles indexed in multiple databases may pop up more than once searching for information, resulting in an overrepresen-  tation of terms.\n",
      "109 The naïve search results were assembled and deduplicated to prevent over-representation.\n",
      "110 Provided that the path  to the directory of search results is given, the import_naive_results function in Ananse automatically ﬁnds each ﬁle’s database and ﬁle  type, selects analogous columns, and joins them to form a single dataset.\n",
      "111 This function imports the search results from a speciﬁed  path.\n",
      "112 If the parameters clean_dataset and save_dataset are set to TRUE , the function deduplicates search results after importing and  saves the full search results to a csv ﬁle.\n",
      "113 The parameter save_directory contains the path to a directory where search results will be  saved.\n",
      "114 If save_dataset is set to TRUE while the parameter save_directory is set to the directory of choice, the merged ﬁle is saved to that  directory path containing the naive search results ﬁles.\n",
      "115 After the results are obtained, a pandas data frame consisting of assembled  search results is returned.\n",
      "116 After the merging, Ananse performs deduplication based on the article titles and abstracts and returns  diﬀerent articles.\n",
      "117 In this instance, Ananse removes the exact title duplicates; titles that are over 95% similar or abstract that are more \n",
      "118 Screenshot of deduplicated ﬁles in csv format \n",
      "119 than 90% similar are removed.\n",
      "120 The user can change these similarity levels.\n",
      "121 Ananse returned 6,786 distinct articles out of the 7,809  articles fed into it and created a csv ﬁle, a screenshot of it is as shown in Fig.\n",
      "122 Ananse automatically corrected and classiﬁed 100% of the 1023 duplicate articles identiﬁed by manual deduplication.\n",
      "123 Extracting and identifying keywords \n",
      "124 Ananse uses the Rapid Automatic Keyword Extraction (RAKE) [35] , a keyword extraction method, to extract potential keywords  from the titles, keywords and abstracts of articles in the deduplicated dataset.\n",
      "125 The RAKE is designed to identify keywords in sci-  entiﬁc literature by selecting strings of words uninterrupted using a list of stopwords (6 + ) and phrase delimiters (punctuation) to  detect the most relevant words or phrases in a piece of text [36] .\n",
      "126 The function extract_terms call the RAKE algorithm and eliminates  keywords that only appear in a single article and excludes phrases with only one word from the list of potential keywords result-  ing in a more precise search.\n",
      "127 Ananse then combines the author- and database-tagged keywords with the search terms.\n",
      "128 The author  and database tagged keywords are combined as dictionary objects created with extract_terms to deﬁne all possible keywords.\n",
      "129 All the  possible keywords are then passed to a function create_dtm for function wrapping, which generates a document-feature matrix using  the potential keywords as features and the combined titles, abstracts, and keywords of each article (also referred to as noted) as the  documents.\n",
      "130 The selection of keywords using the frequency of occurrence can be a good indicator of the relevance of a word/term to a search  strategy.\n",
      "131 However, we moved beyond this and generated a keyword co ‐occurrence network.\n",
      "132 The co-occurrence network creates and  measures each term’s importance and inﬂuence in relation to the topic being reviewed [37] .\n",
      "133 Using the document matrix containing  the potential keywords, we generated a keyword co-occurrence network.\n",
      "134 Each keyword is represented by a point referred to as the  node, and an edge also represents a link between the keywords.\n",
      "135 Each node represents a potential search term, and the edges are co-  occurrences of two terms in a study’s title, abstract, or tagged keywords [37] .\n",
      "136 In Ananse , the co-occurrence network is implemented  with the function create_network, which measures the importance of each term in relation to the selected topic being reviewed.\n",
      "137 The function get_centrality is used to evaluate the node importance of a graph and returns a dictionary containing nodes with their  importance.\n",
      "138 Figure 5 shows a co-occurrence network with important keywords closely grouped.\n",
      "139 The dense region shows keywords that are \n",
      "140 Identifying important nodes using a full network \n",
      "141 Important  nodes  represent  keywords  to  be  used  to  generate  ﬁnal  search  terms.\n",
      "142 Two  methods  to  identify  impor-  tant  nodes  were  explored  in  Ananse :  ﬁtting  a  spline  model  to  the  node  importance  to  select  tipping  points  and  cu-  mulative  approach,  which  ﬁnds  the  minimum  number  of  nodes  to  capture  a  large  percentage  of  the  total  impor-  tance  of  the  network.\n",
      "143 One  can  decide  which  method  to  use  depending  on  the  distribution  and  preference.\n",
      "144 In  choos-  ing  a  method,  the  ﬁrst  thing  to  do  is  to  look  at  the  distribution  of  node  importance.\n",
      "145 In  Ananse,  the  distribu-  tion  was  plotted  with  the  function  plot_degree_distribution,  plot_rank_degree_distribution,  or  plot_degree_histogram  as  shown  in  Fig.\n",
      "146 A spline model for ﬁnding cut-oﬀ is an appropriate method to identify the cut-oﬀ threshold for keyword importance if the rank  distribution plot has a lot of weak nodes with a long tail.\n",
      "147 On the other hand, the cumulative approach is more appropriate when  there are no clear breaks in the data.\n",
      "148 In Ananse, the ﬁnd_cutoﬀ function ﬁnds the cut-oﬀ for a graph network using either cumulative  or spline method of cutting the degree distribution, as shown in Fig.\n",
      "149 The reduce_graph function then generates a graph consisting  of only important nodes, after which the get_keyword function extracts the keywords from the reduced network.\n",
      "150 Ananse uses the node strength to generate relevant keywords from which the experts can now select their ﬁnal keywords.\n",
      "151 In  this research, Ananse generated 4,596 keywords.\n",
      "152 A screenshot of it is shown in Fig.\n",
      "153 Afterward, the researchers manually reviewed each word or phrase using their expert knowledge to arrive at the ﬁnal  keywords.\n",
      "154 The ﬁnal list of search terms (listed as search strings) was grouped under three concepts, as shown in Table 2 .\n",
      "155 Evidence synthesis has become an essential feature of the current academic landscape, although a lack of transparency often  hampers the process.\n",
      "156 This research reports on the methods used to select search terms that form the building block for performing  evidence synthesis and oﬀers a transparent approach to understand underlying assumptions.\n",
      "157 In systematic reviews, the selection of  key search terms is considered the basic building block for the successful assemblage of knowledge in a particular ﬁeld.\n",
      "158 However, this  process is often left to researchers’ discretion, leaving room for biases and a subjective selection process, aﬀecting the outcomes of  eﬀective evidence synthesis.\n",
      "159 In this research, we designed and implemented a partially automated keyword search software package  using Python for SR to enhance eﬃciency, maximize transparency and comprehensiveness while minimizing subjectivity and bias.\n",
      "160 Dubbed Ananse , our tool provides an eﬃcient and standardized method for developing search strategies using NLP and co-occurrence  networks to identify relevant search terms.\n",
      "161 Our approach combines expert knowledge with a quasi ‐automated method which enhances search recall.\n",
      "162 This is very important  for ﬁelds such as ecology, where non ‐standardized or nuanced terminology or a lack of formal ontologies exist for conducting SRs  [22] .\n",
      "163 Most importantly, Ananse signiﬁcantly reduces the time required to conduct a SR by decreasing time spent on search strategy  development and tedious tasks like assembling and deduplication.\n",
      "164 Compared with the manual process of assembling results, Ananse  reduced by more than half the time required to assembly results.\n",
      "165 Similarly, while it took two of the co-authors two days of full-time  work to remove duplicates, Ananse removed the duplicates eﬃciently in about a minute or less and achieved 100% accuracy.\n",
      "166 With  the reduction in time needed to develop a search strategy and assemble and deduplicate the results, our approach makes extensive  systematic reviews and meta ‐analyses more eﬃcient and eﬀective compared with conventional approaches.\n",
      "167 Our research contributes  to the emergence and application of an ever-growing set of tools and software that can be used to facilitate transparent, reproducible  reviews and develop reproducible synthesis workﬂows such as metaDigitise [38] , litsearchr [22] in R, and revtools [39] .\n",
      "168 These eﬀorts  should help facilitate the reproducibility of ecological reviews, enhance transparency, and improve the rigor of evidence used to guide  policy decisions [40] .\n",
      "169 In its current implementation , Ananse, a Python package, contains a suite of functions to improve the eﬃciency of keywords  selection for systematic reviews.\n",
      "170 For instance, by automatically deduplicating and assembling results from separate databases, Ananse  provides a systematic approach to facilitate knowledge synthesis through SR.\n",
      "171 Also, apart from generating keywords, it can act as  middleware or a data converter for integrating multiple datasets into a database.\n",
      "172 Done manually, this is a time ‐intensive process \n",
      "173 because platforms and databases export results in diﬀerent formats [2] .\n",
      "174 Furthermore, we used the agile method of software engineering  with open-source software development, thereby making Ananse easily customizable and improved upon as researchers use it beyond  the application to cumulative eﬀects assessments.\n",
      "175 Currently, Ananse has a popularity of 131 downloads per week on the Python  Package Index ( https://snyk.io/advisor/python/ananse ).\n",
      "176 Ananse contributes to the development of open-source software systems  needed to speed up systematic review.\n",
      "177 In its current state, Ananse provides a means to merge and deduplicate keywords for experts  programmatically.\n",
      "178 By its design and implementation, Ananse allows researchers to modify their requirements without creating new  software.\n",
      "179 Even though Ananse has been used for a cumulative eﬀect use case [30] , it is general-purpose software for a systematic  review of any kind.\n",
      "180 It can be applied broadly in ecology and evolutionary biology as well as other ﬁelds.\n",
      "181 Compared to conventional approaches for developing keywords for systematic review, our method is far eﬀective and eﬃcient by  signiﬁcantly reducing the time and resources needed to develop search strategies to conduct systematic reviews.\n",
      "182 Ananse substantially  reduces the time spent on the systematic review by automating time-consuming tasks such as assembling and deduplicating large  search results.\n",
      "183 Ananse saves time and enhances eﬀective keyword generation compared to traditional methods by automating the  tedious and bias-prone aspect of systematic review tasks.\n",
      "184 Therefore, Ananse presents an approach to performing large systematic  reviews within a short period of time.\n",
      "185 Our results can be used as a starting point to frame future studies according to well-deﬁned terminology.\n",
      "186 Future research would  enhance the front-end of Ananse through a user-friendly graphical interface.\n",
      "187 Currently, Ananse allows one user per time; this func-  tionality can be improved by making Ananse a server-type software with capabilities to permit concurrent and multi-user interaction.\n",
      "188 The requirements would be modiﬁed as we get feedback from the research community.\n",
      "189 Ranked Node Strength with cut-oﬀ points) \n",
      "190 The CFS Cumulative Eﬀects Program supported this work.\n",
      "191 Natural Resources Canada for their instrumental roles in moving the study forward.\n",
      "192 Declaration of Competing Interest \n",
      "193 The authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to \n",
      "194 inﬂuence the work reported in this paper.\n",
      "195 This research was funded by Natural Resources Canada (Canadian Forest Service, Forest Ecosystem Integrity and Cumulative  Eﬀects Programs).\n",
      "196 Natural Resources Canada (Canadian Forest Service, Forest Ecosystem Integrity, and Cumulative Eﬀects Programs)  did not play any role in preparing this manuscript.\n",
      "197 CochraneEvidence Synthesis - What is it and why do we need it?, Cochrane, 2021 https://www.cochrane.org/news/evidence-synthesis-what-it-and-why-do-we-need-it \n",
      "198 N.R. Haddaway, M.J. Westgate, Predicting the time needed for environmental systematic reviews and systematic maps, Conservation Biology 33 (2019) 2 \n",
      "199 Why systematic reviews matter.\n",
      "200 D.T. Blumstein, Systematic reviews and maps as tools for applying behavioral \n",
      "201 Legese, Method for conducting systematic literature review and meta-analysis for environmental science research, MethodsX 7 \n",
      "202 I.J. Marshall, B.C. Wallace, Toward systematic review automation: a practical guide to using machine learning tools in research synthesis, Systematic reviews 8 \n",
      "203 Gartlehner, Resource use during systematic review production varies \n",
      "204 Hanson, W.D. Kearns, J.J. Orriola, R.L. Polo, K.A. Sakmar, It takes longer than you think: librarian time spent on systematic review \n",
      "205 Using text mining for study identiﬁcation in systematic reviews: a systematic \n",
      "206 Evaluating Abstrackr, a semi-automated online screening program for systematic \n",
      "207 Thomas, Pinpointing needles in giant haystacks: use of text mining to reduce \n",
      "208 Turchin, L.F. Florez Builes, Using Natural Language Processing to Measure and Improve Quality of Diabetes Care: A Systematic Review, Journal of Diabetes \n",
      "209 Ghasemian, Natural Language Processing Systems for Diagnosing and Determining Level of Lung Cancer: A Systematic \n",
      "210 E.M. Grames, A.N. Stillman, M.W. Tingley, C.S. Elphick, An automated approach to identifying search terms for systematic reviews using keyword co-occurrence \n",
      "211 Testing a citation and text-based framework for retrieving publications for literature reviews.\n",
      "212 Grames, Stillman, Introduction to litsearchr with an example of writing a systematic review search strategy for black-backed woodpecker occupancy of post-ﬁre \n",
      "213 P.C. Lee, H.N. Su, Investigating the structure of regional innovation system research through keyword co-occurrence and social network analysis, Innovation 12 \n",
      "214 M.J. Westgate, revtools: An R package to support article screening for evidence synthesis, Research synthesis methods 10 (4) (2019) 606–614 .\n",
      "215 E.M. Grames, C.S. Elphick, Use of study design principles would increase the reproducibility of reviews in conservation biology, Biological Conservation 241 \n",
      "216 Simon, Routine development of objectively derived search strategies, Systematic reviews 1 (1) (2012) 1–10 .\n",
      "217 Hsin-ning Su, Pei-Chun Lee, Mapping knowledge structure by keyword co-occurrence: A ﬁrst look at journal papers in Technology Foresight, Scientometrics 85 \n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for s in valid_sentences:\n",
    "    counter += 1\n",
    "    print(counter, s.sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The origin file is: <__main__.Document object at 0x7fc3eaeb5d90>\n",
      "The sentence is:\n",
      "These eﬀorts  should help facilitate the reproducibility of ecological reviews, enhance transparency, and improve the rigor of evidence used to guide  policy decisions [40] .\n",
      "The inventory holds:\n",
      "{'DET': 3, 'NOUN': 8, 'SPACE': 2, 'AUX': 1, 'VERB': 6, 'ADP': 2, 'ADJ': 1, 'PUNCT': 3, 'CCONJ': 1, 'PART': 1, 'X': 3}\n",
      "The token details are:\n",
      "[('These', 'DET'), ('eﬀorts', 'NOUN'), (' ', 'SPACE'), ('should', 'AUX'), ('help', 'VERB'), ('facilitate', 'VERB'), ('the', 'DET'), ('reproducibility', 'NOUN'), ('of', 'ADP'), ('ecological', 'ADJ'), ('reviews', 'NOUN'), (',', 'PUNCT'), ('enhance', 'VERB'), ('transparency', 'NOUN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('improve', 'VERB'), ('the', 'DET'), ('rigor', 'NOUN'), ('of', 'ADP'), ('evidence', 'NOUN'), ('used', 'VERB'), ('to', 'PART'), ('guide', 'VERB'), (' ', 'SPACE'), ('policy', 'NOUN'), ('decisions', 'NOUN'), ('[', 'X'), ('40', 'X'), (']', 'X'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "valid_sentences[-50].summarize(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Clustering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
