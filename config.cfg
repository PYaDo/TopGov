# Description: Configuration file for the project


[preprocessing]
# The tokenizer is used to tokenize the sentences within the Sent.py class.
# The tokenizer can be any of the transformer models from spaCy.
# An overview of the models can be found here: https://spacy.io/models/en
TOKENIZER = en_core_web_trf